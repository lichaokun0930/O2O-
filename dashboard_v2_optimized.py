# -*- coding: utf-8 -*-
"""
O2Oé—¨åº—æ•°æ®åˆ†æçœ‹æ¿ v2.1 - ä¼˜åŒ–ç‰ˆæœ¬
åŸºäºDash + Plotlyæ„å»ºçš„å¯è§†åŒ–æ•°æ®çœ‹æ¿

ä¼˜åŒ–å†…å®¹ï¼š
- âœ… åˆ é™¤AIåˆ†ææ¨¡å—
- âœ… æ·»åŠ æ•°æ®ç¼“å­˜æœºåˆ¶ï¼ˆæå‡åŠ è½½é€Ÿåº¦5-10å€ï¼‰
- âœ… è§„èŒƒåŒ–æ—¥å¿—ç³»ç»Ÿï¼ˆä¾¿äºé—®é¢˜æ’æŸ¥ï¼‰
- âœ… ä¿®å¤KPIè®¡ç®—çš„ç¡¬ç¼–ç åˆ—ç´¢å¼•ï¼ˆé¿å…æ•°æ®é”™ä½ï¼‰

è¿è¡Œæ–¹å¼ï¼š
    python dashboard_v2_optimized.py
"""

import dash
from dash import dcc, html, Input, Output, State, callback, dash_table, ALL
from dash.exceptions import PreventUpdate
import dash_bootstrap_components as dbc
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np
from pathlib import Path
import os
from datetime import datetime
import base64
import io
import pickle
import hashlib
import logging
from logging.handlers import RotatingFileHandler

# å¯¼å…¥é—¨åº—åˆ†æå™¨ï¼ˆé›†æˆuntitled1.pyåŠŸèƒ½ï¼‰
from store_analyzer import get_store_analyzer

# PDFç”Ÿæˆç›¸å…³åº“
from reportlab.lib.pagesizes import A4, landscape
from reportlab.lib.units import inch, cm
from reportlab.pdfgen import canvas
from reportlab.lib.utils import ImageReader
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib import colors
from PIL import Image

# ==================== æ—¥å¿—ç³»ç»Ÿé…ç½® ====================
def setup_logger(name='dashboard', level=logging.INFO):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    logger = logging.getLogger(name)
    logger.setLevel(level)
    
    # é¿å…é‡å¤æ·»åŠ handler
    if logger.handlers:
        return logger
    
    # åˆ›å»ºlogsç›®å½•
    log_dir = Path('logs')
    log_dir.mkdir(exist_ok=True)
    
    # æ§åˆ¶å°è¾“å‡º
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    
    # æ–‡ä»¶è¾“å‡ºï¼ˆè‡ªåŠ¨è½®è½¬ï¼Œæœ€å¤§10MBï¼Œä¿ç•™5ä¸ªå¤‡ä»½ï¼‰
    file_handler = RotatingFileHandler(
        log_dir / 'dashboard.log',
        maxBytes=10*1024*1024,
        backupCount=5,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    file_handler.setFormatter(file_formatter)
    
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger

# åˆå§‹åŒ–æ—¥å¿—
logger = setup_logger()

# å…¨å±€é…ç½®
DEFAULT_REPORT_PATH = "./reports/æ·®å®‰ç”Ÿæ€æ–°åŸå•†å“10.29 çš„å‰¯æœ¬_åˆ†ææŠ¥å‘Š.xlsx"
APP_TITLE = "O2Oé—¨åº—æ•°æ®åˆ†æçœ‹æ¿ v2.1"

# ==================== æ•°æ®ç¼“å­˜å·¥å…· ====================
class DataCache:
    """æ•°æ®ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir='./cache'):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        logger.info(f"ç¼“å­˜ç›®å½•: {self.cache_dir.absolute()}")
    
    def _get_file_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def _get_cache_path(self, file_path):
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        file_hash = self._get_file_hash(file_path)
        return self.cache_dir / f"{Path(file_path).stem}_{file_hash}.cache"
    
    def get(self, file_path):
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        try:
            cache_path = self._get_cache_path(file_path)
            
            if not cache_path.exists():
                logger.debug(f"ç¼“å­˜ä¸å­˜åœ¨: {cache_path.name}")
                return None
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆæ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼‰
            cache_mtime = cache_path.stat().st_mtime
            file_mtime = Path(file_path).stat().st_mtime
            
            if cache_mtime < file_mtime:
                logger.info(f"ç¼“å­˜å·²è¿‡æœŸ: {cache_path.name}")
                cache_path.unlink()  # åˆ é™¤è¿‡æœŸç¼“å­˜
                return None
            
            # åŠ è½½ç¼“å­˜
            with open(cache_path, 'rb') as f:
                data = pickle.load(f)
            
            logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜æ•°æ®: {cache_path.name}")
            return data
            
        except Exception as e:
            logger.warning(f"è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def set(self, file_path, data):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        try:
            cache_path = self._get_cache_path(file_path)
            
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            cache_size = cache_path.stat().st_size / 1024 / 1024  # MB
            logger.info(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {cache_path.name} ({cache_size:.2f}MB)")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def clear(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        count = 0
        for cache_file in self.cache_dir.glob('*.cache'):
            cache_file.unlink()
            count += 1
        logger.info(f"ğŸ—‘ï¸ å·²æ¸…é™¤ {count} ä¸ªç¼“å­˜æ–‡ä»¶")

# åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
data_cache = DataCache()

# ==================== KPIåˆ—åæ˜ å°„é…ç½® ====================
class KPIColumnMapping:
    """KPIåˆ—åæ˜ å°„é…ç½® - é¿å…ç¡¬ç¼–ç ç´¢å¼•"""
    
    # KPI Sheetçš„åˆ—åæ˜ å°„
    KPI_COLUMNS = {
        'é—¨åº—': ['é—¨åº—', 'Store', 'åº—é“º'],
        'æ€»SKUæ•°(å«è§„æ ¼)': ['æ€»SKUæ•°(å«è§„æ ¼)', 'æ€»SKUæ•°', 'Total SKU'],
        'å•è§„æ ¼SPUæ•°': ['å•è§„æ ¼SPUæ•°', 'å•è§„æ ¼å•†å“æ•°'],
        'å•è§„æ ¼SKUæ•°': ['å•è§„æ ¼SKUæ•°'],
        'å¤šè§„æ ¼SKUæ€»æ•°': ['å¤šè§„æ ¼SKUæ€»æ•°', 'å¤šè§„æ ¼SKUæ•°'],
        'æ€»SKUæ•°(å»é‡å)': ['æ€»SKUæ•°(å»é‡å)', 'å»é‡SKUæ•°', 'Unique SKU'],
        'åŠ¨é”€SKUæ•°': ['åŠ¨é”€SKUæ•°', 'åŠ¨é”€æ•°', 'Active SKU'],
        'æ»é”€SKUæ•°': ['æ»é”€SKUæ•°', 'æ»é”€æ•°', 'Inactive SKU'],
        'æ€»é”€å”®é¢(å»é‡å)': ['æ€»é”€å”®é¢(å»é‡å)', 'æ€»é”€å”®é¢', 'Total Revenue'],
        'åŠ¨é”€ç‡': ['åŠ¨é”€ç‡', 'Active Rate'],
        'å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°': ['å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°', 'å¤šè§„æ ¼å•†å“æ•°']
    }
    
    # åˆ†ç±»Sheetçš„åˆ—åæ˜ å°„
    CATEGORY_COLUMNS = {
        'ä¸€çº§åˆ†ç±»': ['ä¸€çº§åˆ†ç±»', 'ç¾å›¢ä¸€çº§åˆ†ç±»', 'Category'],
        'çˆ†å“æ•°': ['ç¾å›¢ä¸€çº§åˆ†ç±»çˆ†å“skuæ•°', 'çˆ†å“æ•°', 'Hot Products'],
        'æŠ˜æ‰£': ['ç¾å›¢ä¸€çº§åˆ†ç±»æŠ˜æ‰£', 'æŠ˜æ‰£', 'Discount'],
        'æŠ˜æ‰£SKUæ•°': ['ç¾å›¢ä¸€çº§åˆ†ç±»æŠ˜æ‰£skuæ•°', 'æŠ˜æ‰£å•†å“æ•°'],
        'å»é‡SKUæ•°': ['ç¾å›¢ä¸€çº§åˆ†ç±»å»é‡SKUæ•°(å£å¾„åŒåŠ¨é”€ç‡)', 'å»é‡SKUæ•°'],
        'åŠ¨é”€SKUæ•°': ['ç¾å›¢ä¸€çº§åˆ†ç±»åŠ¨é”€skuæ•°', 'åŠ¨é”€æ•°'],
        'åŠ¨é”€ç‡': ['ç¾å›¢ä¸€çº§åˆ†ç±»åŠ¨é”€ç‡(ç±»å†…)', 'åŠ¨é”€ç‡']
    }
    
    @classmethod
    def find_column(cls, df, standard_name, mapping_dict):
        """
        åœ¨DataFrameä¸­æŸ¥æ‰¾åˆ—å
        
        Args:
            df: DataFrame
            standard_name: æ ‡å‡†åˆ—å
            mapping_dict: åˆ—åæ˜ å°„å­—å…¸
            
        Returns:
            å®é™…åˆ—åï¼Œå¦‚æœæ‰¾ä¸åˆ°è¿”å›None
        """
        if standard_name not in mapping_dict:
            return None
        
        possible_names = mapping_dict[standard_name]
        
        for name in possible_names:
            if name in df.columns:
                return name
        
        return None
    
    @classmethod
    def safe_get_value(cls, df, row_idx, standard_name, mapping_dict, default=0):
        """
        å®‰å…¨è·å–DataFrameä¸­çš„å€¼
        
        Args:
            df: DataFrame
            row_idx: è¡Œç´¢å¼•
            standard_name: æ ‡å‡†åˆ—å
            mapping_dict: åˆ—åæ˜ å°„å­—å…¸
            default: é»˜è®¤å€¼
            
        Returns:
            åˆ—å€¼ï¼Œå¦‚æœæ‰¾ä¸åˆ°è¿”å›é»˜è®¤å€¼
        """
        col_name = cls.find_column(df, standard_name, mapping_dict)
        
        if col_name is None:
            logger.warning(f"æœªæ‰¾åˆ°åˆ—: {standard_name} (å°è¯•: {mapping_dict.get(standard_name, [])})")
            return default
        
        try:
            if row_idx >= len(df):
                return default
            return df.iloc[row_idx][col_name]
        except Exception as e:
            logger.error(f"è·å–å€¼å¤±è´¥ [{standard_name}]: {e}")
            return default

# ==================== æ•°æ®åŠ è½½å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰====================
class DataLoader:
    """æ•°æ®åŠ è½½å™¨ - è´Ÿè´£ä»ExcelæŠ¥å‘Šä¸­è¯»å–å’Œé¢„å¤„ç†æ•°æ®"""
    
    def __init__(self, excel_path, use_cache=True):
        self.excel_path = excel_path
        self.use_cache = use_cache
        self.data = {}
        self.load_all_data()
    
    def load_all_data(self):
        """åŠ è½½æ‰€æœ‰sheetæ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        try:
            # å°è¯•ä»ç¼“å­˜åŠ è½½
            if self.use_cache:
                cached_data = data_cache.get(self.excel_path)
                if cached_data is not None:
                    self.data = cached_data
                    logger.info(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½æ•°æ®: {Path(self.excel_path).name}")
                    self._log_data_summary()
                    return
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»ExcelåŠ è½½
            logger.info(f"ğŸ“‚ ä»ExcelåŠ è½½æ•°æ®: {Path(self.excel_path).name}")
            
            # è·å–æ‰€æœ‰sheetåç§°
            excel_file = pd.ExcelFile(self.excel_path)
            sheet_names = excel_file.sheet_names
            logger.debug(f"å¯ç”¨Sheet: {sheet_names}")
            
            # å®šä¹‰Sheetåç§°æ˜ å°„è¡¨
            sheet_mapping = {
                'kpi': ['æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”', 'KPI', 'æ ¸å¿ƒæŒ‡æ ‡'],
                'role_analysis': ['å•†å“è§’è‰²åˆ†æ', 'è§’è‰²åˆ†æ'],
                'price_analysis': ['ä»·æ ¼å¸¦åˆ†æ', 'ä»·æ ¼åˆ†æ'],
                'category_l1': ['ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡', 'ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡', 'ä¸€çº§åˆ†ç±»'],
                'sku_details': ['è¯¦ç»†SKUæŠ¥å‘Š(å»é‡å)', 'SKUæŠ¥å‘Š', 'è¯¦ç»†SKUæŠ¥å‘Š']
            }
            
            # éå†æ‰€æœ‰Sheetï¼ŒæŒ‰åç§°åŒ¹é…
            for key, possible_names in sheet_mapping.items():
                for sheet_name in sheet_names:
                    if any(name in sheet_name for name in possible_names):
                        self.data[key] = pd.read_excel(self.excel_path, sheet_name=sheet_name)
                        logger.debug(f"âœ… åŠ è½½ {key}: '{sheet_name}'")
                        
                        # ç‰¹æ®Šå¤„ç†ï¼šæ¸…ç†ä»·æ ¼å¸¦æ•°æ®
                        if key == 'price_analysis' and not self.data[key].empty:
                            if 'Unnamed' in str(self.data[key].columns[0]):
                                self.data[key] = self.data[key].drop(self.data[key].columns[0], axis=1)
                        break
            
            # åŠ è½½æˆæœ¬åˆ†æç›¸å…³Sheetï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            for sheet_name in sheet_names:
                if 'æˆæœ¬åˆ†ææ±‡æ€»' in sheet_name:
                    self.data['cost_summary'] = pd.read_excel(self.excel_path, sheet_name=sheet_name)
                    logger.debug(f"âœ… åŠ è½½æˆæœ¬åˆ†ææ±‡æ€»æ•°æ®")
                elif 'é«˜æ¯›åˆ©å•†å“' in sheet_name:
                    self.data['high_margin_products'] = pd.read_excel(self.excel_path, sheet_name=sheet_name)
                    logger.debug(f"âœ… åŠ è½½é«˜æ¯›åˆ©å•†å“æ•°æ®")
                elif 'ä½æ¯›åˆ©é¢„è­¦' in sheet_name:
                    self.data['low_margin_warning'] = pd.read_excel(self.excel_path, sheet_name=sheet_name)
                    logger.debug(f"âœ… åŠ è½½ä½æ¯›åˆ©é¢„è­¦æ•°æ®")
            
            # å¡«å……ç¼ºå¤±çš„æ•°æ®
            for key in ['kpi', 'category_l1', 'role_analysis', 'price_analysis', 'sku_details', 
                        'cost_summary', 'high_margin_products', 'low_margin_warning']:
                if key not in self.data:
                    self.data[key] = pd.DataFrame()
            
            # ä¿å­˜åˆ°ç¼“å­˜
            if self.use_cache:
                data_cache.set(self.excel_path, self.data)
            
            self._log_data_summary()
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}", exc_info=True)
            # åˆ›å»ºç©ºæ•°æ®æ¡†ä½œä¸ºå¤‡ç”¨
            self.data = {
                'kpi': pd.DataFrame(),
                'category_l1': pd.DataFrame(),
                'role_analysis': pd.DataFrame(),
                'price_analysis': pd.DataFrame()
            }
    
    def _log_data_summary(self):
        """è®°å½•æ•°æ®æ‘˜è¦"""
        logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å®Œæˆ:")
        logger.info(f"  - KPIæ•°æ®: {self.data['kpi'].shape}")
        logger.info(f"  - åˆ†ç±»æ•°æ®: {self.data['category_l1'].shape}")
        logger.info(f"  - ä»·æ ¼å¸¦æ•°æ®: {self.data['price_analysis'].shape}")
        logger.info(f"  - è§’è‰²åˆ†æ: {self.data['role_analysis'].shape}")
    
    def get_kpi_summary(self):
        """è·å–KPIæ‘˜è¦æ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆ - ä½¿ç”¨åˆ—åæ˜ å°„ï¼‰"""
        if self.data['kpi'].empty:
            logger.warning("KPIæ•°æ®ä¸ºç©º")
            return {}
        
        kpi_df = self.data['kpi']
        if len(kpi_df) == 0:
            return {}
        
        summary = {}
        mapper = KPIColumnMapping
        
        # ä½¿ç”¨åˆ—åæ˜ å°„å®‰å…¨è·å–å€¼
        summary['é—¨åº—'] = mapper.safe_get_value(
            kpi_df, 0, 'é—¨åº—', mapper.KPI_COLUMNS, default='æœªçŸ¥'
        )
        summary['æ€»SKUæ•°(å«è§„æ ¼)'] = mapper.safe_get_value(
            kpi_df, 0, 'æ€»SKUæ•°(å«è§„æ ¼)', mapper.KPI_COLUMNS, default=0
        )
        summary['å•è§„æ ¼SPUæ•°'] = mapper.safe_get_value(
            kpi_df, 0, 'å•è§„æ ¼SPUæ•°', mapper.KPI_COLUMNS, default=0
        )
        summary['å•è§„æ ¼SKUæ•°'] = mapper.safe_get_value(
            kpi_df, 0, 'å•è§„æ ¼SKUæ•°', mapper.KPI_COLUMNS, default=0
        )
        summary['å¤šè§„æ ¼SKUæ€»æ•°'] = mapper.safe_get_value(
            kpi_df, 0, 'å¤šè§„æ ¼SKUæ€»æ•°', mapper.KPI_COLUMNS, default=0
        )
        summary['æ€»SKUæ•°(å»é‡å)'] = mapper.safe_get_value(
            kpi_df, 0, 'æ€»SKUæ•°(å»é‡å)', mapper.KPI_COLUMNS, default=0
        )
        summary['åŠ¨é”€SKUæ•°'] = mapper.safe_get_value(
            kpi_df, 0, 'åŠ¨é”€SKUæ•°', mapper.KPI_COLUMNS, default=0
        )
        summary['æ»é”€SKUæ•°'] = mapper.safe_get_value(
            kpi_df, 0, 'æ»é”€SKUæ•°', mapper.KPI_COLUMNS, default=0
        )
        summary['æ€»é”€å”®é¢(å»é‡å)'] = mapper.safe_get_value(
            kpi_df, 0, 'æ€»é”€å”®é¢(å»é‡å)', mapper.KPI_COLUMNS, default=0
        )
        summary['åŠ¨é”€ç‡'] = mapper.safe_get_value(
            kpi_df, 0, 'åŠ¨é”€ç‡', mapper.KPI_COLUMNS, default=0
        )
        summary['å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°'] = mapper.safe_get_value(
            kpi_df, 0, 'å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°', mapper.KPI_COLUMNS, default=0
        )
        
        # ä»ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡ä¸­è·å–é—¨åº—çˆ†å“æ•°å’Œå¹³å‡æŠ˜æ‰£
        if not self.data['category_l1'].empty:
            category_df = self.data['category_l1']
            
            # ä½¿ç”¨åˆ—åæ˜ å°„è·å–çˆ†å“æ•°
            burst_col = mapper.find_column(category_df, 'çˆ†å“æ•°', mapper.CATEGORY_COLUMNS)
            if burst_col:
                summary['é—¨åº—çˆ†å“æ•°'] = category_df[burst_col].sum()
            else:
                summary['é—¨åº—çˆ†å“æ•°'] = 0
            
            # ä½¿ç”¨åˆ—åæ˜ å°„è·å–æŠ˜æ‰£
            discount_col = mapper.find_column(category_df, 'æŠ˜æ‰£', mapper.CATEGORY_COLUMNS)
            if discount_col:
                discount_values = pd.to_numeric(category_df[discount_col], errors='coerce')
                summary['é—¨åº—å¹³å‡æŠ˜æ‰£'] = discount_values.mean()
            else:
                summary['é—¨åº—å¹³å‡æŠ˜æ‰£'] = 10.0
        
        # ========== æ–°å¢æŒ‡æ ‡è®¡ç®— ==========
        if not self.data['sku_details'].empty:
            sku_df = self.data['sku_details']
            
            # 1. å¹³å‡SKUå•ä»·
            if len(sku_df.columns) > 1:
                price_col = pd.to_numeric(sku_df.iloc[:, 1], errors='coerce')
                summary['å¹³å‡SKUå•ä»·'] = price_col.mean()
            
            # 2. é«˜ä»·å€¼SKUå æ¯”
            if len(sku_df.columns) > 1 and summary.get('æ€»SKUæ•°(å»é‡å)', 0) > 0:
                high_value_count = (pd.to_numeric(sku_df.iloc[:, 1], errors='coerce') > 50).sum()
                total_skus = summary['æ€»SKUæ•°(å»é‡å)']
                summary['é«˜ä»·å€¼SKUå æ¯”'] = (high_value_count / total_skus) if total_skus > 0 else 0
            
            # 3. çˆ†æ¬¾é›†ä¸­åº¦
            if len(sku_df.columns) > 2 and summary.get('æ€»é”€å”®é¢(å»é‡å)', 0) > 0:
                price_col = pd.to_numeric(sku_df.iloc[:, 1], errors='coerce').fillna(0)
                sales_col = pd.to_numeric(sku_df.iloc[:, 2], errors='coerce').fillna(0)
                sku_df_temp = sku_df.copy()
                sku_df_temp['revenue'] = price_col * sales_col
                
                top10_revenue = sku_df_temp.nlargest(10, 'revenue')['revenue'].sum()
                total_revenue = summary['æ€»é”€å”®é¢(å»é‡å)']
                summary['çˆ†æ¬¾é›†ä¸­åº¦'] = (top10_revenue / total_revenue) if total_revenue > 0 else 0
        
        # 5. ä¿ƒé”€å¼ºåº¦
        if not self.data['category_l1'].empty:
            category_df = self.data['category_l1']
            
            discount_sku_col = mapper.find_column(category_df, 'æŠ˜æ‰£SKUæ•°', mapper.CATEGORY_COLUMNS)
            dedup_sku_col = mapper.find_column(category_df, 'å»é‡SKUæ•°', mapper.CATEGORY_COLUMNS)
            
            if discount_sku_col and dedup_sku_col:
                total_discount_skus = pd.to_numeric(category_df[discount_sku_col], errors='coerce').sum()
                total_dedup_skus = pd.to_numeric(category_df[dedup_sku_col], errors='coerce').sum()
                summary['ä¿ƒé”€å¼ºåº¦'] = (total_discount_skus / total_dedup_skus) if total_dedup_skus > 0 else 0
        
        # ========== æˆæœ¬åˆ†æKPI ==========
        if not self.data.get('cost_summary', pd.DataFrame()).empty:
            cost_df = self.data['cost_summary']
            if len(cost_df) > 0:
                total_row = cost_df.iloc[0]
                
                if 'æˆæœ¬é”€å”®é¢' in cost_df.columns:
                    summary['æ€»æˆæœ¬é”€å”®é¢'] = total_row['æˆæœ¬é”€å”®é¢']
                
                if 'æ¯›åˆ©' in cost_df.columns:
                    summary['æ€»æ¯›åˆ©'] = total_row['æ¯›åˆ©']
                
                if 'ç¾å›¢ä¸€çº§åˆ†ç±»å”®ä»·æ¯›åˆ©ç‡' in cost_df.columns:
                    summary['å¹³å‡æ¯›åˆ©ç‡'] = total_row['ç¾å›¢ä¸€çº§åˆ†ç±»å”®ä»·æ¯›åˆ©ç‡']
        
        if not self.data.get('high_margin_products', pd.DataFrame()).empty:
            summary['é«˜æ¯›åˆ©å•†å“æ•°'] = len(self.data['high_margin_products'])
        
        logger.debug(f"KPIæ‘˜è¦è®¡ç®—å®Œæˆï¼Œå…±{len(summary)}ä¸ªæŒ‡æ ‡")
        return summary
    
    def get_category_analysis(self):
        """è·å–åˆ†ç±»åˆ†ææ•°æ®"""
        return self.data['category_l1']
    
    def get_role_analysis(self):
        """è·å–å•†å“è§’è‰²åˆ†ææ•°æ®"""
        return self.data['role_analysis']
    
    def get_price_analysis(self):
        """è·å–ä»·æ ¼å¸¦åˆ†ææ•°æ®"""
        return self.data['price_analysis']


# ==================== é—¨åº—ç®¡ç†å™¨ ====================
class StoreManager:
    """é—¨åº—ç®¡ç†å™¨ - æ”¯æŒå¤šé—¨åº—åˆ†æä¸åˆ‡æ¢"""
    
    def __init__(self):
        self.stores = {}
        self.current_store = None
        self.default_report = DEFAULT_REPORT_PATH
    
    def add_store(self, name, report_path):
        """æ·»åŠ é—¨åº—"""
        self.stores[name] = report_path
        if not self.current_store:
            self.current_store = name
        logger.info(f"âœ… é—¨åº—ã€{name}ã€‘å·²æ·»åŠ ")
    
    def get_store_list(self):
        """è·å–æ‰€æœ‰é—¨åº—åˆ—è¡¨"""
        stores = list(self.stores.keys())
        if Path(self.default_report).exists():
            default_name = "é»˜è®¤é—¨åº—"
            if default_name not in stores:
                stores.insert(0, default_name)
        return stores
    
    def get_report_path(self, name):
        """è·å–é—¨åº—æŠ¥å‘Šè·¯å¾„"""
        if name in self.stores:
            return self.stores[name]
        elif name == "é»˜è®¤é—¨åº—":
            return self.default_report
        return None
    
    def switch_store(self, name):
        """åˆ‡æ¢å½“å‰é—¨åº—"""
        report_path = self.get_report_path(name)
        if report_path and Path(report_path).exists():
            self.current_store = name
            return DataLoader(report_path)
        return None
    
    def clear_all(self):
        """æ¸…é™¤æ‰€æœ‰é—¨åº—"""
        self.stores.clear()
        self.current_store = None
        logger.info("ğŸ—‘ï¸ å·²æ¸…é™¤æ‰€æœ‰é—¨åº—")


# ==================== æ™ºèƒ½å¸ƒå±€ç®¡ç†å™¨ ====================
class SmartLayoutManager:
    """æ™ºèƒ½å¸ƒå±€ç®¡ç†å™¨ - æ ¹æ®æ•°æ®å¤æ‚åº¦è‡ªåŠ¨è°ƒæ•´å›¾è¡¨å°ºå¯¸"""
    
    @staticmethod
    def calculate_heatmap_dimensions(data):
        """è®¡ç®—çƒ­åŠ›å›¾æœ€ä¼˜å°ºå¯¸"""
        if data.empty:
            return 900, 600
        
        rows = len(data)
        cols = len(data.columns) if hasattr(data, 'columns') else 1
        
        base_width = 900
        base_height = max(600, rows * 30 + 200)
        
        max_width = 1400
        max_height = 900
        
        width = min(base_width, max_width)
        height = min(base_height, max_height)
        
        return width, height
    
    @staticmethod
    def calculate_pie_dimensions(categories):
        """è®¡ç®—é¥¼å›¾æœ€ä¼˜å°ºå¯¸"""
        num_categories = len(categories) if categories else 4
        
        if num_categories <= 4:
            return 700, 700
        elif num_categories <= 8:
            return 800, 800
        else:
            return 900, 900
    
    @staticmethod
    def calculate_bar_dimensions(data_length):
        """è®¡ç®—æŸ±çŠ¶å›¾æœ€ä¼˜å°ºå¯¸"""
        base_height = 600
        if data_length > 10:
            base_height = 700
        if data_length > 15:
            base_height = 800
        
        return 1000, base_height
