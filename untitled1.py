# -*- coding: utf-8 -*-
"""
é—¨åº—åŸºç¡€æ•°æ®åˆ†æå·¥å…· v3.4 - æœ¬åœ°è¿è¡Œç‰ˆ
python untitled1.py --inputs "ä½ çš„Excelæ–‡ä»¶è·¯å¾„.xlsx"
ã€è¿è¡ŒæŒ‡ä»¤ã€‘
PowerShellä¸­æ‰§è¡Œï¼š
        cd "D:\\Python1\\O2O_Analysis\\O2Oæ•°æ®åˆ†æ"
        .\\.venv\\Scripts\\python.exe é—¨åº—åŸºç¡€æ•°æ®åˆ†æ\\untitled1.py

æˆ–è€…åœ¨VS Codeç»ˆç«¯ä¸­æ‰§è¡Œï¼š
    & D:\\Python1\\O2O_Analysis\\O2Oæ•°æ®åˆ†æ\\.venv\\Scripts\\python.exe d:/Python1/O2O_Analysis/O2Oæ•°æ®åˆ†æ/é—¨åº—åŸºç¡€æ•°æ®åˆ†æ/untitled1.py

ã€åŠŸèƒ½è¯´æ˜ã€‘
æœ¬è„šæœ¬æ¥æºäº Google Colabï¼Œå·²ä¼˜åŒ–ä¸ºæœ¬åœ°/VS Code ç¯å¢ƒå¯ç›´æ¥è¿è¡Œçš„ç‰ˆæœ¬ã€‚
ä¸»è¦åŠŸèƒ½ï¼š
1. é—¨åº—å•†å“ç»“æ„åˆ†æï¼ˆSKU/SPUç»Ÿè®¡ã€å¤šè§„æ ¼å•†å“è¯†åˆ«ï¼‰
2. ä»·æ ¼å¸¦åˆ†æã€å•†å“è§’è‰²åˆ†æ
3. ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡åˆ†æ
4. å¤šè§„æ ¼å•†å“æŠ¥å‘Šç”Ÿæˆ
5. æ•°æ®ä¸€è‡´æ€§æ ¡éªŒ

ã€è¾“å‡ºæŠ¥å‘Šã€‘
- æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”ï¼šåŒ…å«æ€»SKUæ•°ã€å•è§„æ ¼SKUæ•°ã€å¤šè§„æ ¼SKUæ€»æ•°ç­‰å…³é”®æŒ‡æ ‡
- å¤šè§„æ ¼å•†å“æŠ¥å‘Š(å…¨)ï¼šæ‰€æœ‰è¢«è¯†åˆ«ä¸ºå¤šè§„æ ¼çš„å•†å“è¯¦ç»†æ¸…å•
- å”¯ä¸€å¤šè§„æ ¼å•†å“åˆ—è¡¨ï¼šå»é‡åçš„å¤šè§„æ ¼å•†å“ï¼ŒåŒ…å«åˆ†ç±»å’Œæœˆå”®ä¿¡æ¯
- ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡ï¼šå„åˆ†ç±»çš„è¯¦ç»†ç»Ÿè®¡æ•°æ®

ä¼˜åŒ–è¦ç‚¹ï¼ˆä¸æ”¹å˜ä¸šåŠ¡é€»è¾‘/éœ€æ±‚ï¼‰ï¼š
- å»é™¤ files.upload ä¸ /content è·¯å¾„ä¾èµ–ï¼Œæ”¯æŒå‘½ä»¤è¡Œä¼ å…¥æˆ–äº¤äº’å¼è¾“å…¥æœ¬åœ°æ–‡ä»¶è·¯å¾„
- è¡¥é½å¿…è¦å¯¼å…¥ä¸å¥å£®æ€§æ ¡éªŒï¼ˆæ–‡ä»¶å­˜åœ¨ã€Excel é”æ–‡ä»¶ã€æ•°å€¼åˆ—ç±»å‹ï¼‰
- Excel å¯¼å‡ºä¼˜å…ˆ xlsxwriterï¼Œä¸å¯ç”¨æ—¶å›é€€ openpyxl
- ç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼šå¤šè§„æ ¼å•†å“æŠ¥å‘Š(å…¨)è¡Œæ•° = æ ¸å¿ƒæŒ‡æ ‡ä¸­å¤šè§„æ ¼SKUæ€»æ•°
"""

import os
import sys
import argparse
import traceback
from pathlib import Path
import pandas as pd
import numpy as np
import datetime as dt
import re

# ----------------------------------------
# 2. æ ¸å¿ƒå‡½æ•°å®šä¹‰ (ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒï¼Œä¿æŒå®Œæ•´æ€§)
# ----------------------------------------

def assign_product_role(row):
    """æ ¹æ®ä»·æ ¼å¸¦å’Œé”€é‡ä¸ºå•†å“åˆ†é…è§’è‰²ã€‚"""
    if pd.isna(row['price_band']): return 'åŠ£åŠ¿å“'
    if row['price_band'] in ['0-5 å…ƒ', '5-10 å…ƒ'] and row['sales_qty'] > 10: return 'å¼•æµå“'
    if row['price_band'] in ['10-20 å…ƒ', '20-30 å…ƒ', '30-40 å…ƒ', '40-50 å…ƒ', '50-60 å…ƒ', '60-70 å…ƒ', '70-80 å…ƒ', '80-90 å…ƒ'] and row['revenue'] > 50: return 'åˆ©æ¶¦å“'
    if row['price_band'] == '100 å…ƒä»¥ä¸Š': return 'å½¢è±¡å“'
    return 'åŠ£åŠ¿å“'

def assign_consumption_scenarios(df, scenarios_dict):
    """æ ¹æ®å…³é”®è¯ä¸ºå•†å“åˆ†é…æ¶ˆè´¹åœºæ™¯æ ‡ç­¾ã€‚"""
    if 'product_name' not in df.columns or 'l1_category' not in df.columns:
        df['consumption_scenarios'] = [[]] * len(df)
        return df
    def get_scenarios(row):
        product_info = str(row.get('product_name', '')).lower() + str(row.get('l1_category', '')).lower()
        return [scenario for scenario, keywords in scenarios_dict.items() if any(kw.lower() in product_info for kw in keywords)]
    df['consumption_scenarios'] = df.apply(get_scenarios, axis=1)
    return df

def load_and_clean_data(file_path, store_name, scenarios_dict):
    """åŠ è½½ã€æ¸…æ´—å¹¶é¢„å¤„ç†å•ä¸ªé—¨åº—çš„æ•°æ®ï¼Œä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰è¡ç”Ÿåˆ—ã€‚"""
    print(f"\nâš™ï¸  å¼€å§‹å¤„ç†: {store_name} (æ–‡ä»¶: {os.path.basename(file_path)})")
    # åŸºç¡€æ–‡ä»¶æ ¡éªŒ
    p = Path(file_path)
    if not p.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"); return None
    if p.name.startswith('~$'):
        print(f"âŒ æ£€æµ‹åˆ° Excel é”æ–‡ä»¶: {p.name}ï¼Œè¯·å…ˆå…³é—­ Excel åé‡è¯•ã€‚"); return None

    try:
        if p.suffix.lower() == '.csv':
            try:
                df = pd.read_csv(p, on_bad_lines='skip', encoding='utf-8')
            except UnicodeDecodeError:
                df = pd.read_csv(p, on_bad_lines='skip', encoding='gbk')
        else:
            df = pd.read_excel(p)
        print(f"âœ… æ–‡ä»¶ '{p.name}' è¯»å–æˆåŠŸã€‚åŸå§‹è¡Œæ•°: {len(df)}")
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}"); return None

    # Strip whitespace from column names immediately after reading
    df.columns = df.columns.str.strip()
    print(f"â„¹ï¸ è¯»å–æ–‡ä»¶åï¼Œå»é™¤é¦–å°¾ç©ºæ ¼çš„åˆ—å: {list(df.columns)}")

    # More flexible column mapping based on keywords
    mapped_columns = {}
    # Define potential column names in both English and Chinese, prioritizing English if present
    potential_col_names = {
        # åˆ†ç±»
        'l1_category': ['l1_category', 'ä¸€çº§åˆ†ç±»', 'ç¾å›¢ä¸€çº§åˆ†ç±»', 'å¤§ç±»', 'åˆ†ç±»', 'ä¸€çº§å“ç±»'],
        'l3_category': ['l3_category', 'ç¾å›¢ä¸‰çº§åˆ†ç±»', 'ä¸‰çº§åˆ†ç±»', 'å­ç±»', 'ç»†ç±»', 'ä¸‰çº§å“ç±»'],
        'å•†å®¶åˆ†ç±»': ['å•†å®¶åˆ†ç±»'],
        # åŸºç¡€å•†å“ä¿¡æ¯
        'product_name': ['product_name', 'å•†å“åç§°', 'å“å', 'åç§°'],
        'barcode': ['barcode', 'æ¡ç ', 'æ¡å½¢ç ', 'EAN', 'UPC'],
        # ä»·æ ¼/é”€é‡
        'price': ['price', 'å”®ä»·', 'ç°ä»·', 'é”€å”®ä»·', 'ä»·æ ¼'],
        'original_price': ['original_price', 'åŸä»·', 'åˆ’çº¿ä»·', 'å‚è€ƒä»·'],
        'sales_qty': ['sales_qty', 'æœˆå”®', 'é”€é‡', 'æœˆé”€é‡', 'é”€å”®æ•°é‡'],
        # åº“å­˜/è§„æ ¼
        'åº“å­˜': ['åº“å­˜', 'å‰©ä½™åº“å­˜', 'åº“å­˜æ•°', 'åº“å­˜æ•°é‡', 'stock', 'Stock'],
        'è§„æ ¼åç§°': ['è§„æ ¼åç§°', 'è§„æ ¼', 'è§„æ ¼å', 'è§„æ ¼å‹å·', 'è§„æ ¼å€¼', 'spec', 'spec_name', 'variant'],
        # æˆæœ¬ç›¸å…³
        'cost': ['cost', 'æˆæœ¬', 'æˆæœ¬ä»·', 'è¿›ä»·', 'è¿›è´§ä»·', 'é‡‡è´­ä»·', 'å•†å“æˆæœ¬'],
        'store_code': ['store_code', 'åº—å†…ç ', 'å•†å“ç¼–ç ', 'å†…éƒ¨ç¼–ç ', 'å•†å“ä»£ç ', 'é—¨åº—ç¼–ç ', 'åº—é“ºç¼–ç ']
    }

    # Attempt to find the correct column in the dataframe based on potential names
    # Store the mapping from original stripped name to standard name
    reverse_mapped_columns = {}
    for standard_name, potential_names in potential_col_names.items():
        found_col = None
        for name in potential_names:
            if name in df.columns:
                found_col = name
                break # Found a match, move to the next standard name
        if found_col:
            mapped_columns[found_col] = standard_name
            reverse_mapped_columns[standard_name] = found_col # Store mapping from standard back to found original
        else:
             # If a primary essential column is not found, print a warning
            if standard_name in ['product_name', 'price', 'sales_qty', 'l1_category', 'original_price', 'åº“å­˜']:
                 print(f"âš ï¸ æœªæ‰¾åˆ°å¿…è¦åˆ— '{standard_name}' (å°è¯•åç§°: {potential_names})")

    # Rename columns based on the mapping found
    df.rename(columns=mapped_columns, inplace=True)
    print(f"â„¹ï¸ åº”ç”¨åˆ—åæ˜ å°„åï¼Œå½“å‰åˆ—å: {list(df.columns)}")

    # Define essential columns using the *standard* names after mapping
    essential_cols = ['product_name', 'price', 'sales_qty', 'l1_category', 'original_price', 'åº“å­˜']

    print(f"â„¹ï¸ æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨: {essential_cols}")
    df_cols = list(df.columns)
    print(f"â„¹ï¸ DataFrame columns after renaming: {df_cols}")
    print(f"â„¹ï¸ Essential columns list: {essential_cols}")

    # Robust check for essential columns: verify all essential columns are in df.columns
    missing_cols = [col for col in essential_cols if col not in df_cols]
    if missing_cols:
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}"); return None
    print("âœ… å¿…è¦åˆ—æ£€æŸ¥é€šè¿‡ã€‚")

    # Select only the essential columns (and other desired columns) into a new DataFrame
    # Include other potentially useful columns if they were found and mapped
    cols_to_keep = essential_cols + ['è§„æ ¼åç§°', 'l3_category', 'å•†å®¶åˆ†ç±»', 'barcode', 'cost', 'store_code']
    # Filter cols_to_keep to only include those actually present in the DataFrame after renaming
    cols_to_keep_present = [col for col in cols_to_keep if col in df.columns]
    df_processed = df[cols_to_keep_present].copy()

    print(f"â„¹ï¸ åˆ›å»ºæ–°çš„DataFrameï¼ŒåªåŒ…å«ä»¥ä¸‹åˆ—: {list(df_processed.columns)}")

    initial_rows = len(df_processed)

    # æ ‡å‡†åŒ–è§„æ ¼åç§°ï¼Œé¿å…ç©ºç™½/å­—ç¬¦ä¸²"nan"è®¡å…¥è§„æ ¼
    if 'è§„æ ¼åç§°' in df_processed.columns:
        df_processed['è§„æ ¼åç§°'] = df_processed['è§„æ ¼åç§°'].where(~df_processed['è§„æ ¼åç§°'].isna(), None)
        df_processed['è§„æ ¼åç§°'] = df_processed['è§„æ ¼åç§°'].apply(lambda x: x.strip() if isinstance(x, str) else x)
        df_processed.loc[df_processed['è§„æ ¼åç§°'] == '', 'è§„æ ¼åç§°'] = None

    # å…ˆè§„èŒƒåŒ–é”€é‡æ–‡æœ¬ï¼ˆå¦‚ 1.2ä¸‡ã€3åƒã€1,234ã€500+ï¼‰åˆ°çº¯æ•°å­—
    def parse_quantity(val):
        if pd.isna(val):
            return np.nan
        s = str(val).strip()
        if s == "":
            return np.nan
        # å»æ‰é€—å·ä¸åŠ å·
        s = s.replace(',', '').replace('+', '')
        # åŒ¹é…å¸¦å•ä½çš„ä¸­æ–‡æ•°é‡
        m = re.match(r'^(\d+(?:\.\d+)?)\s*([ä¸‡äº¿åƒç™¾wWkK]?)$', s)
        if m:
            num = float(m.group(1))
            unit = m.group(2)
            factor = 1.0
            if unit in ['w', 'W', 'ä¸‡']:
                factor = 10000.0
            elif unit in ['k', 'K', 'åƒ']:
                factor = 1000.0
            elif unit in ['ç™¾']:
                factor = 100.0
            elif unit in ['äº¿']:
                factor = 100000000.0
            return num * factor
        # çº¯æ•°å­—æˆ–å…¶ä»–å¯è§£ææ ·å¼
        try:
            return float(s)
        except Exception:
            return np.nan

    if 'sales_qty' in df_processed.columns:
        df_processed['sales_qty'] = df_processed['sales_qty'].apply(parse_quantity)
    # æ•°å€¼åŒ–å¹¶å°†ç¼ºå¤±å€¼å¡« 0ï¼Œé¿å…å›  NaN è¢«ä¸¢å¼ƒå¯¼è‡´åŠ¨é”€ç»Ÿè®¡å¤±çœŸ
    for col in ['price', 'sales_qty', 'original_price', 'åº“å­˜']:
        if col in df_processed.columns:
            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce').fillna(0)
    print(f"â„¹ï¸ è½¬æ¢å¿…è¦æ•°å€¼åˆ—ä¸ºæ•°å€¼ç±»å‹å®Œæˆï¼Œå¹¶å°†ç¼ºå¤±å€¼å¡«å……ä¸º 0ã€‚")

    # ä»…å¯¹çœŸæ­£å¿…éœ€çš„æ ‡è¯†æ€§/åˆ†ç»„æ€§å­—æ®µåšéç©ºæ ¡éªŒï¼Œé¿å…æŠŠ 0 é”€å”®/ä»·æ ¼çš„SKUç›´æ¥ä¸¢å¼ƒ
    df_processed.dropna(subset=['product_name', 'l1_category'], inplace=True)
    rows_after_dropna = len(df_processed)
    print(f"â„¹ï¸ ç§»é™¤å…³é”®æ ‡è¯†/åˆ†ç±»ç©ºå€¼è¡Œåï¼Œå‰©ä½™è¡Œæ•°: {rows_after_dropna} (ç§»é™¤äº† {initial_rows - rows_after_dropna} è¡Œ)")

    initial_rows_after_dropna = len(df_processed)
    if 'l1_category' in df_processed.columns: # Check if the column exists before filtering
        df_processed = df_processed[df_processed['l1_category'] != 'åº—é“ºç®¡ç†'].copy()
    rows_after_filter = len(df_processed)
    print(f"â„¹ï¸ ç§»é™¤ 'l1_category' ä¸º 'åº—é“ºç®¡ç†' çš„è¡Œã€‚å‰©ä½™è¡Œæ•°: {rows_after_filter} (ç§»é™¤äº† {initial_rows_after_dropna - rows_after_filter} è¡Œ)")


    if df_processed.empty:
        print("âš ï¸ ç»è¿‡æ¸…æ´—åï¼ŒDataFrame ä¸ºç©ºã€‚")
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    df_processed['revenue'] = df_processed['price'] * df_processed['sales_qty']
    df_processed['original_price_revenue'] = df_processed['original_price'] * df_processed['sales_qty']
    # Add a check before calculating discount to avoid division by zero or issues with original_price being NaN/0
    df_processed['discount'] = 0 # Default discount to 0
    valid_price_mask = (df_processed['original_price'] > 0) & (df_processed['price'] >= 0) # Ensure original_price is positive and price is non-negative
    df_processed.loc[valid_price_mask, 'discount'] = (df_processed['original_price'] - df_processed['price']) / df_processed['original_price']
    df_processed.loc[df_processed['discount'] < 0, 'discount'] = 0 # Ensure discount is not negative


    df_processed['Store'] = store_name
    print("â„¹ï¸ è®¡ç®—è¡ç”Ÿåˆ—å®Œæˆ (revenue, original_price_revenue, discount, Store)ã€‚")

    price_bins = [0, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, np.inf]
    price_labels = ['0-5 å…ƒ', '5-10 å…ƒ', '10-20 å…ƒ', '20-30 å…ƒ', '30-40 å…ƒ', '40-50 å…ƒ', '50-60 å…ƒ', '60-70 å…ƒ', '70-80 å…ƒ', '80-90 å…ƒ', '100 å…ƒä»¥ä¸Š']
    df_processed['price_band'] = pd.cut(df_processed['price'], bins=price_bins, labels=price_labels, right=False, include_lowest=True)
    print("â„¹ï¸ åˆ†é…ä»·æ ¼å¸¦å®Œæˆã€‚")

    df_processed['role'] = df_processed.apply(assign_product_role, axis=1)
    print("â„¹ï¸ åˆ†é…å•†å“è§’è‰²å®Œæˆã€‚")

    df_processed = assign_consumption_scenarios(df_processed, scenarios_dict)
    print("â„¹ï¸ åˆ†é…æ¶ˆè´¹åœºæ™¯å®Œæˆã€‚")

    df_all_skus = df_processed.copy()
    
    # è°ƒè¯•ï¼šæ£€æŸ¥æˆæœ¬åˆ—æ˜¯å¦å­˜åœ¨
    if 'cost' in df_all_skus.columns:
        cost_count = df_all_skus['cost'].notna().sum()
        print(f"â„¹ï¸ æˆæœ¬æ•°æ®æ£€æµ‹ï¼šå…± {cost_count} æ¡SKUåŒ…å«æˆæœ¬æ•°æ®")
    
    # ä½¿ç”¨å¤šçº§æ’åºè¿›è¡Œå»é‡ï¼šé”€é‡é™åºã€ä»·æ ¼å‡åºã€åº“å­˜é™åºã€è§„æ ¼åç§°å‡åºï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    # ç¡®ä¿å¯¹äºç›¸åŒé”€é‡çš„å¤šè§„æ ¼å•†å“ï¼Œä¼˜å…ˆé€‰æ‹©ä»·æ ¼ä½ã€åº“å­˜é«˜çš„è§„æ ¼ä½œä¸ºä»£è¡¨
    sort_columns = ['sales_qty', 'price', 'åº“å­˜']
    sort_ascending = [False, True, False]
    
    # å¦‚æœå­˜åœ¨è§„æ ¼åç§°åˆ—ï¼Œåˆ™åŠ å…¥æ’åº
    if 'è§„æ ¼åç§°' in df_processed.columns:
        sort_columns.append('è§„æ ¼åç§°')
        sort_ascending.append(True)
    
    df_deduplicated = df_processed.sort_values(
        by=sort_columns, 
        ascending=sort_ascending,
        na_position='last'
    ).drop_duplicates(subset=['product_name'], keep='first').copy()
    df_active = df_deduplicated[df_deduplicated['sales_qty'] > 0].copy()

    print(f"âœ… æ¸…æ´—å®Œæˆ: å…± {len(df_all_skus)} SKU (å«è§„æ ¼), å»é‡å {len(df_deduplicated)} SKU, å…¶ä¸­åŠ¨é”€ {len(df_active)} SKUã€‚")
    return df_all_skus, df_deduplicated, df_active

# ====== å¤šè§„æ ¼è¯†åˆ«è¾…åŠ©ï¼šä»å•†å“åç§°è§£æè§„æ ¼ï¼Œå¹¶å½’ä¸€åŒ–åŸºå ======
def _extract_inferred_spec(name: str) -> str:
    if not isinstance(name, str) or not name:
        return ''
    s = name.lower()
    specs = []
    # æ•°é‡*è§„æ ¼ï¼Œå¦‚ 12*50g, 6Ã—500ml
    m_iter = re.findall(r'(\d+\s*[xÃ—*]\s*\d+\s*(?:g|kg|ml|l|ç‰‡|åŒ…|è¢‹|æ”¯|æš|ç“¶|å¬|å·)?)', s)
    specs.extend([re.sub(r'\s+', '', m) for m in m_iter])
    # ä½“ç§¯/é‡é‡ï¼Œå¦‚ 500ml, 1.5l, 300g, 2kg
    m_iter = re.findall(r'(\d+(?:\.\d+)?\s*(?:ml|l|g|kg))', s)
    specs.extend([re.sub(r'\s+', '', m) for m in m_iter])
    # è®¡æ•°å•ä½ï¼Œå¦‚ 12ç‰‡, 6åŒ…, 24æ”¯
    m_iter = re.findall(r'(\d+\s*(?:ç‰‡|åŒ…|è¢‹|æ”¯|æš|ç“¶|å¬|ç›’|å·|å—|ç‰‡è£…|è¢‹è£…|æ”¯è£…))', s)
    specs.extend([re.sub(r'\s+', '', m) for m in m_iter])
    # å£å‘³/å˜ä½“å…³é”®è¯ï¼ˆç®€ç‰ˆï¼‰
    flavor_kw = [
        'åŸå‘³','è‰è“','é¦™è‰','å·§å…‹åŠ›','æŸ æª¬','èŠ’æœ','æ©™','è“è“','é’æŸ ','è‘¡è„','å¯ä¹','é›¶åº¦','ä¹Œé¾™','èŒ‰è‰','å¥¶ç»¿',
        'å¾®è¾£','ä¸­è¾£','ç‰¹è¾£','éº»è¾£','æ¸…çˆ½','æ— ç³–','ä½ç³–','0ç³–','å°‘ç³–','æ— ç›','ä½ç›','æµ·ç›','é»‘ç³–','çº¢ç³–','ç‡•éº¦','å…¨éº¦','ä½è„‚','é«˜é’™','é«˜è›‹ç™½',
        'å¤§','ä¸­','å°','è¿·ä½ ','mini','å®¶åº­è£…','åˆ†äº«è£…','é‡è´©','åŠ å¤§','åŠ åš','ä¾¿æº'
    ]
    for kw in flavor_kw:
        if kw in s:
            specs.append(kw)
    # åˆå¹¶ä¸ºå»é‡æœ‰åºå­—ç¬¦ä¸²
    if not specs:
        return ''
    uniq = []
    for t in specs:
        if t and t not in uniq:
            uniq.append(t)
    return ' '.join(uniq)

def _normalize_base_name(name: str) -> str:
    if not isinstance(name, str) or not name:
        return ''
    s = name.lower()
    # å»æ‰æ‹¬å·ä¸­çš„å†…å®¹ï¼ˆå¸¸ä¸ºå£å‘³/è§„æ ¼ï¼‰
    s = re.sub(r'[\(ï¼ˆ\[][^\)ï¼‰\]]*[\)ï¼‰\]]', '', s)
    # å»æ‰æ•°é‡*è§„æ ¼ã€æ•°å­—+å•ä½ç­‰
    s = re.sub(r'\d+\s*[xÃ—*]\s*\d+\s*(?:g|kg|ml|l|ç‰‡|åŒ…|è¢‹|æ”¯|æš|ç“¶|å¬|å·)?', '', s)
    s = re.sub(r'\d+(?:\.\d+)?\s*(?:ml|l|g|kg)', '', s)
    s = re.sub(r'\d+\s*(?:ç‰‡|åŒ…|è¢‹|æ”¯|æš|ç“¶|å¬|ç›’|å·|å—|ç‰‡è£…|è¢‹è£…|æ”¯è£…)', '', s)
    # å»æ‰å¸¸è§å˜ä½“å…³é”®è¯
    variant_kw = ['åŸå‘³','è‰è“','é¦™è‰','å·§å…‹åŠ›','æŸ æª¬','èŠ’æœ','å¾®è¾£','ä¸­è¾£','ç‰¹è¾£','æ— ç³–','ä½ç³–','0ç³–','å®¶åº­è£…','åˆ†äº«è£…','é‡è´©','è¿·ä½ ','mini','å¤§','ä¸­','å°']
    for kw in variant_kw:
        s = s.replace(kw, '')
    # å»æ‰å¤šä½™ç©ºç™½ä¸æ ‡ç‚¹
    s = re.sub(r'[^\u4e00-\u9fff0-9a-zA-Z]+', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def identify_multi_spec_products(df):
    """è¯†åˆ«å¤šè§„æ ¼å•†å“ã€‚"""
    if df is None or df.empty:
        return pd.DataFrame()
    if 'product_name' not in df.columns:
        return pd.DataFrame()

    work = df.copy()
    # æ ‡å‡†åŒ–è§„æ ¼åˆ—
    if 'è§„æ ¼åç§°' in work.columns:
        work['è§„æ ¼åç§°'] = work['è§„æ ¼åç§°'].where(~work['è§„æ ¼åç§°'].isna(), None)
        work['è§„æ ¼åç§°'] = work['è§„æ ¼åç§°'].apply(lambda x: x.strip() if isinstance(x, str) else x)
        work.loc[work['è§„æ ¼åç§°'] == '', 'è§„æ ¼åç§°'] = None
    else:
        work['è§„æ ¼åç§°'] = None

    # ä»åç§°æ¨æ–­è§„æ ¼ï¼Œå¹¶ç”ŸæˆåŸºå
    work['inferred_spec'] = work['product_name'].apply(_extract_inferred_spec)
    work['base_name'] = work['product_name'].apply(_normalize_base_name)

    # ç»„é”®ï¼ˆé—¨åº—ä¼˜å…ˆï¼‰
    has_store = 'Store' in work.columns
    key_pn = ['Store', 'product_name'] if has_store else ['product_name']
    key_base = ['Store', 'base_name'] if has_store else ['base_name']

    # ä¿¡å·1ï¼šåŒä¸€ product_name ä¸‹çš„éç©ºè§„æ ¼åç§°>1
    sig1 = work.dropna(subset=['è§„æ ¼åç§°']).groupby(key_pn)['è§„æ ¼åç§°'].nunique(dropna=True)
    sig1_keys = sig1[sig1 > 1].index

    # ä¿¡å·2ï¼šåŒä¸€ base_name ä¸‹çš„ inferred_spec>1
    sig2 = work[work['inferred_spec'] != ''].groupby(key_base)['inferred_spec'].nunique()
    sig2_keys = sig2[sig2 > 1].index

    # ä¿¡å·3ï¼šåŒä¸€ base_name ä¸‹æ¡ç å¤šå€¼ï¼ˆä¸”å•†å“åä¸å®Œå…¨ç›¸åŒï¼Œé¿å…åŒæ¡é‡å¤ï¼‰
    if 'barcode' in work.columns:
        tmp = work.copy()
        tmp['barcode'] = tmp['barcode'].astype(str)
        sig3 = tmp.groupby(key_base)['barcode'].nunique()
        sig3_keys = sig3[sig3 > 1].index
    else:
        sig3_keys = []

    def idx_to_df(keys, cols):
        if isinstance(keys, pd.MultiIndex):
            return keys.to_frame(index=False).rename(columns={0: cols[0], 1: cols[1]} if len(cols) == 2 else {0: cols[0]})
        else:
            return pd.DataFrame({cols[0]: list(keys)})

    key_pn_df = idx_to_df(sig1_keys, key_pn)
    key_base_df_2 = idx_to_df(sig2_keys, key_base)
    key_base_df_3 = idx_to_df(sig3_keys, key_base)

    # æ”¶é›†æ‰€æœ‰è¢«è¯†åˆ«ä¸ºå¤šè§„æ ¼çš„base_nameï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    all_multi_base_names = set()
    
    # ä»ä¿¡å·1ï¼ˆè§„æ ¼åˆ—ï¼‰æå–base_name
    if not key_pn_df.empty:
        # é¢„å…ˆå»ºç«‹product_nameåˆ°base_nameçš„æ˜ å°„ï¼Œé¿å…é‡å¤æŸ¥è¯¢
        if has_store:
            pn_to_base_map = work.set_index(['Store', 'product_name'])['base_name'].to_dict()
            for _, row in key_pn_df.iterrows():
                key = (row['Store'], row['product_name'])
                if key in pn_to_base_map:
                    all_multi_base_names.add((row['Store'], pn_to_base_map[key]))
        else:
            pn_to_base_map = work.set_index('product_name')['base_name'].to_dict()
            for _, row in key_pn_df.iterrows():
                if row['product_name'] in pn_to_base_map:
                    all_multi_base_names.add(pn_to_base_map[row['product_name']])
    
    # ä»ä¿¡å·2ï¼ˆåç§°è§£æï¼‰æå–base_name
    if not key_base_df_2.empty:
        for _, row in key_base_df_2.iterrows():
            if has_store:
                all_multi_base_names.add((row['Store'], row['base_name']))
            else:
                all_multi_base_names.add(row['base_name'])
    
    # ä»ä¿¡å·3ï¼ˆæ¡ç å¤šå€¼ï¼‰æå–base_name
    if not key_base_df_3.empty:
        for _, row in key_base_df_3.iterrows():
            if has_store:
                all_multi_base_names.add((row['Store'], row['base_name']))
            else:
                all_multi_base_names.add(row['base_name'])
    
    if not all_multi_base_names:
        return pd.DataFrame()
    
    # ä½¿ç”¨å‘é‡åŒ–æ“ä½œç­›é€‰ç»“æœï¼Œé¿å…å¤šæ¬¡å¾ªç¯
    if has_store:
        # åˆ›å»ºä¸€ä¸ªæ ‡è®°åˆ—æ¥æ ‡è¯†å¤šè§„æ ¼å•†å“
        work['is_multi_spec'] = work.apply(
            lambda row: (row['Store'], row['base_name']) in all_multi_base_names, 
            axis=1
        )
    else:
        work['is_multi_spec'] = work['base_name'].isin(all_multi_base_names)
    
    result = work[work['is_multi_spec']].copy()
    result = result.drop('is_multi_spec', axis=1)
    
    if result.empty:
        return pd.DataFrame()
    # ä¸ºå®Œæ•´çš„ç»“æœè®¡ç®—è§„æ ¼ç§ç±»æ•°å’Œå¤šè§„æ ¼ä¾æ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    # å˜ä½“é”®ï¼šä¼˜å…ˆ è§„æ ¼åç§°ï¼Œå…¶æ¬¡ inferred_specï¼Œå†æ¬¡ barcode
    def _coalesce_variant(row):
        for c in ['è§„æ ¼åç§°', 'inferred_spec', 'barcode']:
            v = row.get(c, None)
            if isinstance(v, str):
                v = v.strip()
            if v not in (None, '', 'nan') and not (isinstance(v, float) and np.isnan(v)):
                return v
        return None
    
    print(f"â„¹ï¸ å¼€å§‹è®¡ç®—å˜ä½“é”®...")
    result['variant_key'] = result.apply(_coalesce_variant, axis=1)
    
    print(f"â„¹ï¸ å¼€å§‹è®¡ç®—è§„æ ¼ç§ç±»æ•°...")
    # ä½¿ç”¨æ›´ç®€å•çš„è§„æ ¼ç§ç±»æ•°è®¡ç®—æ–¹æ³•
    if has_store:
        vk_cnt = result.dropna(subset=['variant_key']).groupby(['Store', 'base_name'])['variant_key'].nunique().reset_index()
        vk_cnt.columns = ['Store', 'base_name', 'è§„æ ¼ç§ç±»æ•°']
        result = result.merge(vk_cnt, on=['Store', 'base_name'], how='left')
    else:
        vk_cnt = result.dropna(subset=['variant_key']).groupby('base_name')['variant_key'].nunique().reset_index()
        vk_cnt.columns = ['base_name', 'è§„æ ¼ç§ç±»æ•°']
        result = result.merge(vk_cnt, on='base_name', how='left')
    
    result['è§„æ ¼ç§ç±»æ•°'] = result['è§„æ ¼ç§ç±»æ•°'].fillna(2)  # è‡³å°‘ä¸º2çš„å¤šè§„æ ¼å‡è®¾
    
    print(f"â„¹ï¸ å¼€å§‹æ·»åŠ å¤šè§„æ ¼ä¾æ®...")
    # ç®€åŒ–å¤šè§„æ ¼ä¾æ®çš„è®¡ç®—
    def get_trigger_for_row(row):
        triggers = []
        
        if has_store:
            store_name = row['Store']
            base_name = row['base_name']
            product_name = row['product_name']
            
            # æ£€æŸ¥ä¿¡å·æ¥æºï¼ˆç®€åŒ–ç‰ˆï¼‰
            if not key_pn_df.empty and any((key_pn_df['Store'] == store_name) & (key_pn_df['product_name'] == product_name)):
                triggers.append('è§„æ ¼åˆ—')
            if not key_base_df_2.empty and any((key_base_df_2['Store'] == store_name) & (key_base_df_2['base_name'] == base_name)):
                triggers.append('åç§°è§£æ')
            if not key_base_df_3.empty and any((key_base_df_3['Store'] == store_name) & (key_base_df_3['base_name'] == base_name)):
                triggers.append('æ¡ç å¤šå€¼')
        else:
            base_name = row['base_name']
            product_name = row['product_name']
            
            if not key_pn_df.empty and product_name in key_pn_df['product_name'].values:
                triggers.append('è§„æ ¼åˆ—')
            if not key_base_df_2.empty and base_name in key_base_df_2['base_name'].values:
                triggers.append('åç§°è§£æ')
            if not key_base_df_3.empty and base_name in key_base_df_3['base_name'].values:
                triggers.append('æ¡ç å¤šå€¼')
        
        return ', '.join(triggers) if triggers else 'æœªçŸ¥'
    
    # åªå¯¹å‰1000è¡Œè®¡ç®—å¤šè§„æ ¼ä¾æ®ï¼Œé¿å…æ€§èƒ½é—®é¢˜
    if len(result) > 1000:
        result['å¤šè§„æ ¼ä¾æ®'] = 'æ‰¹é‡è¯†åˆ«'
        print(f"âš ï¸ ç”±äºæ•°æ®é‡è¾ƒå¤§({len(result)}è¡Œ)ï¼Œç®€åŒ–å¤šè§„æ ¼ä¾æ®æ ‡æ³¨")
    else:
        result['å¤šè§„æ ¼ä¾æ®'] = result.apply(get_trigger_for_row, axis=1)
    
    print(f"â„¹ï¸ å¤šè§„æ ¼å•†å“è¯†åˆ«å®Œæˆï¼Œå…±{len(result)}è¡Œ")
    return result

def analyze_store_performance(all_skus, deduplicated, active):
    """å¯¹å•ä¸ªé—¨åº—æ•°æ®è¿›è¡Œæ‰€æœ‰ç»´åº¦çš„èšåˆåˆ†æã€‚"""
    if deduplicated.empty:
        print("âš ï¸ deduplicated DataFrame ä¸ºç©ºï¼Œè·³è¿‡åˆ†æã€‚")
        return None
    # ... (çœç•¥ä¸v3.3ç‰ˆå®Œå…¨ç›¸åŒçš„åˆ†æé€»è¾‘) ...
    # (This function is complete and does not need changes)
    store_name = deduplicated['Store'].iloc[0]
    analysis_suite = {}
    
    # ====== æˆæœ¬åˆ†æè®¡ç®— ======
    # æ£€æŸ¥æ˜¯å¦æœ‰æˆæœ¬æ•°æ®
    has_cost_data = 'cost' in all_skus.columns and all_skus['cost'].notna().any()
    
    if has_cost_data:
        print("â„¹ï¸ æ£€æµ‹åˆ°æˆæœ¬æ•°æ®ï¼Œå¼€å§‹è®¡ç®—æˆæœ¬ç›¸å…³æŒ‡æ ‡...")
        
        # ç¡®ä¿coståˆ—ä¸ºæ•°å€¼å‹
        all_skus['cost'] = pd.to_numeric(all_skus['cost'], errors='coerce').fillna(0)
        
        # SKUçº§æˆæœ¬è®¡ç®—
        all_skus['æˆæœ¬é”€å”®é¢'] = all_skus['cost'] * all_skus['sales_qty']
        all_skus['æ¯›åˆ©'] = all_skus['revenue'] - all_skus['æˆæœ¬é”€å”®é¢']
        
        # å”®ä»·æ¯›åˆ©ç‡ï¼ˆæŒ‰å®é™…å”®ä»·è®¡ç®—ï¼‰
        all_skus['å”®ä»·æ¯›åˆ©ç‡'] = all_skus.apply(
            lambda row: (row['æ¯›åˆ©'] / row['revenue']) if row['revenue'] > 0 else 0, 
            axis=1
        )
        
        # å®šä»·æ¯›åˆ©ç‡ï¼ˆæŒ‰åŸä»·è®¡ç®—ï¼‰
        all_skus['åŸä»·é”€å”®é¢'] = all_skus['original_price'] * all_skus['sales_qty']
        all_skus['å®šä»·æ¯›åˆ©'] = all_skus['original_price_revenue'] - all_skus['æˆæœ¬é”€å”®é¢']
        all_skus['å®šä»·æ¯›åˆ©ç‡'] = all_skus.apply(
            lambda row: ((row['original_price'] - row['cost']) / row['original_price']) 
                        if row['original_price'] > 0 else 0, 
            axis=1
        )
        
        # ä¿ç•™æ—§çš„"æ¯›åˆ©ç‡"åˆ—ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼ˆæŒ‡å‘å”®ä»·æ¯›åˆ©ç‡ï¼‰
        all_skus['æ¯›åˆ©ç‡'] = all_skus['å”®ä»·æ¯›åˆ©ç‡']
        
        # ä»·æ ¼å€ç‡å’ŒåŠ ä»·ç‡
        all_skus['ä»·æ ¼å€ç‡'] = all_skus.apply(
            lambda row: (row['price'] / row['cost']) if row['cost'] > 0 else 0, 
            axis=1
        )
        all_skus['åŠ ä»·ç‡'] = all_skus.apply(
            lambda row: ((row['price'] - row['cost']) / row['cost']) if row['cost'] > 0 else 0, 
            axis=1
        )
        
        print(f"âœ… æˆæœ¬æŒ‡æ ‡è®¡ç®—å®Œæˆï¼š")
        print(f"   - å¹³å‡å”®ä»·æ¯›åˆ©ç‡: {all_skus['å”®ä»·æ¯›åˆ©ç‡'].mean():.2%}")
        print(f"   - å¹³å‡å®šä»·æ¯›åˆ©ç‡: {all_skus['å®šä»·æ¯›åˆ©ç‡'].mean():.2%}")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°æˆæœ¬æ•°æ®ï¼Œè·³è¿‡æˆæœ¬åˆ†æ")
    
    # å…³é”®æŒ‡æ ‡ï¼šç”¨æ›´ç¨³å¥çš„å£å¾„è®¡ç®—ï¼Œé¿å…ç©ºå€¼å¯¼è‡´çš„å¯¹é½/ç©ºç™½
    total_revenue_dedup = float(deduplicated['revenue'].sum())
    # æ€»SKUæ•°(å«è§„æ ¼)å£å¾„è°ƒæ•´ï¼šè·¨åˆ†ç±»å»é‡ + ä»¥â€œå•è§„æ ¼SPUæ•° + å¤šè§„æ ¼SKUæ€»æ•°â€è®¡æ•°
    def _norm(s: str) -> str:
        if not isinstance(s, str):
            return ''
        s = s.strip().lower()
        s = re.sub(r"\s+", " ", s)
        return s
    def _spec_or_infer(row):
        spec = row.get('è§„æ ¼åç§°', None)
        spec = spec if isinstance(spec, str) and spec.strip() != '' else ''
        if not spec:
            spec = _extract_inferred_spec(row.get('product_name', ''))
        return _norm(spec)
    def _sku_key(row):
        bc = row.get('barcode', None)
        if isinstance(bc, (int, float)):
            bc = str(bc)
        if isinstance(bc, str):
            bc = bc.strip()
        if bc and bc.lower() not in ('nan', 'none'):
            return f"bc:{bc}"
        pn = _norm(row.get('product_name', ''))
        sp = _spec_or_infer(row)
        return f"pn:{pn}|sp:{sp}"
    single_spu = 0
    multi_spu = 0
    multi_sku_sum = 0
    try:
        # å˜ä½“é”®ä¸åŸºå
        work = all_skus.copy()
        work['base_name'] = work['product_name'].apply(_normalize_base_name)
        def _variant_key(row):
            v = row.get('è§„æ ¼åç§°', None)
            v = v if isinstance(v, str) and v.strip() != '' else None
            if not v:
                v = _extract_inferred_spec(row.get('product_name', ''))
            if not v:
                bc = row.get('barcode', None)
                bc = str(bc).strip() if isinstance(bc, (int, float, str)) else None
                if bc and bc.lower() not in ('nan', 'none'):
                    v = bc
            return _norm(v) if isinstance(v, str) else v
        work['variant_key'] = work.apply(_variant_key, axis=1)
        # åŸºäº base_name çš„å˜ä½“è®¡æ•°
        vc = work.groupby('base_name')['variant_key'].nunique(dropna=True)
        single_spu = int((vc == 1).sum())
        multi_spu = int((vc > 1).sum())
        multi_sku_sum = int(vc[vc > 1].sum())
        # æœ€ç»ˆå«è§„æ ¼æ€»æ•° = å•è§„æ ¼SPUæ•°(æ¯ä¸ªç®—1) + å¤šè§„æ ¼SKUæ€»æ•°(å„è‡ªå˜ä½“æ•°ç›¸åŠ )
        all_skus_count = single_spu + multi_sku_sum
        # å‚è€ƒï¼šè·¨ç±»å»é‡çš„å”¯ä¸€é”®è®¡æ•°ï¼ˆä¾›æ—¥å¿—æ¯”å¯¹ï¼‰
        unique_keys = all_skus.apply(_sku_key, axis=1)
        uniq_key_cnt = int(unique_keys.nunique())
        dup_diff = int(len(all_skus) - uniq_key_cnt)
        if dup_diff > 0:
            print(f"â„¹ï¸ å«è§„æ ¼å»é‡ï¼šåŸè¡Œæ•° {len(all_skus)} -> å»é‡å {uniq_key_cnt}ï¼ˆè·¨åˆ†ç±»é‡å¤ {dup_diff}ï¼‰ | å£å¾„(å•è§„æ ¼+å¤šè§„æ ¼SKUæ€»æ•°)={all_skus_count}ï¼Œå•è§„æ ¼SPU={single_spu}ï¼Œå¤šè§„æ ¼SPU={multi_spu}ï¼Œå¤šè§„æ ¼SKUæ€»æ•°={multi_sku_sum}")
    except Exception:
        # å…œåº•ï¼šå›é€€ä¸ºåŸå§‹è¡Œæ•°
        all_skus_count = int(len(all_skus))
        single_spu = all_skus_count
        multi_sku_sum = 0
    dedup_count = int(len(deduplicated))
    # åŠ¨é”€æŒ‰ deduplicated çš„é”€é‡>0 è®¡æ•°ï¼Œé¿å…ä¸Šæ¸¸ active å¼‚å¸¸å¯¼è‡´é”™ä½
    sales_series = deduplicated['sales_qty'] if 'sales_qty' in deduplicated.columns else pd.Series([], dtype=float)
    # å…œåº•ï¼šå°†ç¼ºå¤±å€¼è§†ä¸º 0
    sales_series = pd.to_numeric(sales_series, errors='coerce').fillna(0)
    active_count = int((sales_series > 0).sum()) if len(sales_series) else int(len(active))
    inactive_count = max(0, dedup_count - active_count)
    # å¤šè§„æ ¼å”¯ä¸€å•†å“æ•°ï¼šå®‰å…¨è®¡ç®—ï¼Œç¡®ä¿ä¸æŠ¥å‘Š(å…¨)ä¸­çš„æ•°æ®å®Œå…¨ä¸€è‡´
    try:
        multi_spec_df = identify_multi_spec_products(all_skus)
        if not multi_spec_df.empty:
            # å¤šè§„æ ¼SKUæ€»æ•°ï¼šç›´æ¥ä½¿ç”¨è¯†åˆ«ç»“æœçš„è¡Œæ•°ï¼Œç¡®ä¿ä¸"å¤šè§„æ ¼å•†å“æŠ¥å‘Š(å…¨)"å®Œå…¨ä¸€è‡´
            multi_sku_sum = int(len(multi_spec_df))
            # å¤šè§„æ ¼SPUæ•°ï¼šæŒ‰base_nameå»é‡è®¡ç®—
            if 'base_name' in multi_spec_df.columns:
                multi_spu = int(multi_spec_df['base_name'].nunique())
            elif 'product_name' in multi_spec_df.columns:
                multi_spu = int(multi_spec_df['product_name'].nunique())
            else:
                multi_spu = 0
            # å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°ï¼šä¼˜å…ˆæŒ‰ product_name å”¯ä¸€ï¼Œç¼ºå¤±æ—¶é€€å› base_name
            if 'product_name' in multi_spec_df.columns:
                multi_spec_unique = int(multi_spec_df['product_name'].nunique())
            elif 'base_name' in multi_spec_df.columns:
                multi_spec_unique = int(multi_spec_df['base_name'].nunique())
            else:
                multi_spec_unique = int(len(multi_spec_df))
        else:
            multi_sku_sum = 0
            multi_spu = 0
            multi_spec_unique = 0
    except Exception:
        multi_sku_sum = 0
        multi_spu = 0
        multi_spec_unique = 0

    # è®¡ç®—å•è§„æ ¼SKUæ•°ï¼šæ€»SKUæ•° - å¤šè§„æ ¼SKUæ€»æ•°
    single_sku_count = all_skus_count - multi_sku_sum
    
    # æ›´æ–°å•è§„æ ¼SPUæ•°çš„è®¡ç®—é€»è¾‘ï¼Œç¡®ä¿ å•è§„æ ¼SPU + å¤šè§„æ ¼SPU = æ€»SPUæ•°çš„ä¸€è‡´æ€§
    single_spu = max(0, dedup_count - multi_spec_unique)

    kpi_df = pd.DataFrame({
        "æ€»SKUæ•°(å«è§„æ ¼)": [all_skus_count],
        "å•è§„æ ¼SPUæ•°": [single_spu],
        "å•è§„æ ¼SKUæ•°": [single_sku_count],  # æ–°å¢ï¼šå•è§„æ ¼SKUæ•°
        "å¤šè§„æ ¼SKUæ€»æ•°": [multi_sku_sum],  # ä½¿ç”¨ç›´æ¥ç»Ÿè®¡çš„ç»“æœï¼Œç¡®ä¿ä¸æŠ¥å‘Š(å…¨)ä¸€è‡´
        "æ€»SKUæ•°(å»é‡å)": [dedup_count],
        "åŠ¨é”€SKUæ•°": [active_count],
        "æ»é”€SKUæ•°": [inactive_count],
        "æ€»é”€å”®é¢(å»é‡å)": [total_revenue_dedup],
        "åŠ¨é”€ç‡": [active_count / dedup_count if dedup_count > 0 else 0.0],
        "å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°": [multi_spec_unique]
    }, index=[store_name])
    kpi_df.index.name = "é—¨åº—"
    analysis_suite['æ€»ä½“æŒ‡æ ‡'] = kpi_df

    # å…³é”®æ•°å­—æ—¥å¿—ï¼Œä¾¿äºä¸å¯¼å‡ºè¡¨æ ¸å¯¹ (æ›´æ–°ç‰ˆï¼šç¡®ä¿ä¸€è‡´æ€§)
    print(
        f"ğŸ“Œ KPI æ±‡æ€» | é—¨åº—: {store_name} | å«è§„æ ¼: {all_skus_count} | å»é‡: {dedup_count} | åŠ¨é”€: {active_count} | æ»é”€: {inactive_count} | å•è§„æ ¼SKU: {single_sku_count} | å¤šè§„æ ¼SKUæ€»æ•°: {multi_sku_sum} | å¤šè§„æ ¼å”¯ä¸€: {multi_spec_unique} | é”€å”®é¢(å»é‡): {total_revenue_dedup:.2f} | åŠ¨é”€ç‡: {kpi_df['åŠ¨é”€ç‡'].iloc[0]:.2%}"
    )
    # è¯Šæ–­ï¼šå¦‚å‡ºç°åŠ¨é”€=å»é‡/æ»é”€=0ï¼Œæ‰“å°è¿›ä¸€æ­¥ä¿¡æ¯ä¾¿äºæ’æŸ¥
    if dedup_count > 0 and active_count == dedup_count:
        zero_sales_dedup = int((sales_series == 0).sum())
        print(f"ğŸ” è¯Šæ–­æç¤º: åŠ¨é”€SKUæ•°ä¸å»é‡SKUæ•°ç›¸ç­‰ï¼Œæ»é”€ä¸º0ã€‚å»é‡é›†å†…é”€é‡ä¸º0çš„SKUæ•°é‡: {zero_sales_dedup}ã€‚")
        # æŠ½æ ·æ˜¾ç¤ºé”€é‡ä¸º0çš„SKUï¼ˆæœ€å¤š5æ¡ï¼‰
        if zero_sales_dedup > 0 and 'product_name' in deduplicated.columns:
            sample_zero = deduplicated.loc[sales_series == 0, 'product_name'].head(5).tolist()
            print(f"   ç¤ºä¾‹é”€é‡ä¸º0çš„SKUï¼ˆå‰5ï¼‰: {sample_zero}")
    if not active.empty:
        price_analysis = active.groupby('price_band', observed=True).agg(SKUæ•°é‡=('product_name', 'nunique'), é”€å”®é¢=('revenue', 'sum'))
        price_analysis['é”€å”®é¢å æ¯”'] = price_analysis['é”€å”®é¢'] / total_revenue_dedup if total_revenue_dedup > 0 else 0
        price_analysis['SKUå æ¯”'] = price_analysis['SKUæ•°é‡'] / active_count if active_count > 0 else 0
        analysis_suite['ä»·æ ¼å¸¦åˆ†æ'] = price_analysis
        role_analysis = active.groupby('role').agg(SKUæ•°é‡=('product_name', 'nunique'), é”€å”®é¢=('revenue', 'sum'))
        role_analysis['é”€å”®é¢å æ¯”'] = role_analysis['é”€å”®é¢'] / total_revenue_dedup if total_revenue_dedup > 0 else 0
        role_analysis['SKUå æ¯”'] = role_analysis['SKUæ•°é‡'] / active_count if active_count > 0 else 0
        analysis_suite['å•†å“è§’è‰²åˆ†æ'] = role_analysis
        # è½»é‡ä¸€è‡´æ€§æ ¡éªŒï¼šä¸¤å¼ è¡¨çš„æ±‡æ€»åº”ä¸åŠ¨é”€/å»é‡æ€»é¢ä¸€è‡´
        try:
            role_sku_sum = int(pd.to_numeric(role_analysis['SKUæ•°é‡'], errors='coerce').fillna(0).sum())
            price_sku_sum = int(pd.to_numeric(price_analysis['SKUæ•°é‡'], errors='coerce').fillna(0).sum())
            role_rev_sum = float(pd.to_numeric(role_analysis['é”€å”®é¢'], errors='coerce').fillna(0).sum())
            price_rev_sum = float(pd.to_numeric(price_analysis['é”€å”®é¢'], errors='coerce').fillna(0).sum())
            print(f"ğŸ” è§’è‰²åˆ†ææ ¡éªŒ | SKUæ±‡æ€»={role_sku_sum} vs åŠ¨é”€SKUæ•°={active_count} | é”€å”®é¢æ±‡æ€»={role_rev_sum:.2f} vs å»é‡é”€å”®é¢={total_revenue_dedup:.2f}")
            print(f"ğŸ” ä»·æ ¼å¸¦åˆ†ææ ¡éªŒ | SKUæ±‡æ€»={price_sku_sum} vs åŠ¨é”€SKUæ•°={active_count} | é”€å”®é¢æ±‡æ€»={price_rev_sum:.2f} vs å»é‡é”€å”®é¢={total_revenue_dedup:.2f}")
        except Exception as ce:
            print(f"âš ï¸ è§’è‰²/ä»·æ ¼å¸¦æ ¡éªŒå¤±è´¥ï¼š{ce}")
    # å…ˆæŒ‰æ—§æ–¹å¼èšåˆ0åº“å­˜æ•°
    l1_analysis = all_skus.groupby('l1_category').agg(ç¾å›¢ä¸€çº§åˆ†ç±»skuæ•°=('product_name', 'size'), ç¾å›¢ä¸€çº§åˆ†ç±»0åº“å­˜æ•°=('åº“å­˜', lambda x: (x == 0).sum()))
    # ç”¨ä¸â€œæ€»SKUæ•°(å«è§„æ ¼)â€ä¸€è‡´çš„å£å¾„æ›¿æ¢åˆ†ç±»skuæ•°ï¼š
    # æ¯ä¸ª base_name åœ¨åˆ†ç±»å†…çš„å˜ä½“æ•° = variant_key nuniqueï¼ˆä¼˜å…ˆè§„æ ¼åç§°â†’åç§°è§£æâ†’æ¡ç ï¼‰ï¼Œ
    # åˆ†ç±»skuæ•° = sum(max(1, å˜ä½“æ•°))
    work_cat = all_skus.copy()
    work_cat['base_name'] = work_cat['product_name'].apply(_normalize_base_name)
    def _vk_cat(row):
        v = row.get('è§„æ ¼åç§°', None)
        v = v if isinstance(v, str) and v.strip() != '' else None
        if not v:
            v = _extract_inferred_spec(row.get('product_name', ''))
        if not v:
            bc = row.get('barcode', None)
            bc = str(bc).strip() if isinstance(bc, (int, float, str)) else None
            if bc and bc.lower() not in ('nan', 'none'):
                v = bc
        return v
    work_cat['variant_key'] = work_cat.apply(_vk_cat, axis=1)
    
    # ä¸ºæ¯ä¸ª base_name æ ‡è®°ä¸»åˆ†ç±»ï¼ˆé¦–æ¬¡å‡ºç°çš„åˆ†ç±»ï¼‰
    work_cat['primary_category'] = work_cat.groupby('base_name')['l1_category'].transform('first')
    
    # æ ‡è®°æ˜¯å¦ä¸ºè·¨åˆ†ç±»å•†å“ï¼ˆåŒä¸€å•†å“å‡ºç°åœ¨å¤šä¸ªåˆ†ç±»ä¸­ï¼‰
    work_cat['is_cross_category'] = work_cat.groupby('base_name')['l1_category'].transform('nunique') > 1
    
    # åªä¿ç•™ä¸»åˆ†ç±»çš„è®°å½•è¿›è¡Œç»Ÿè®¡ï¼ˆé¿å…è·¨åˆ†ç±»é‡å¤è®¡æ•°ï¼‰
    work_cat_dedup = work_cat[work_cat['l1_category'] == work_cat['primary_category']].copy()
    
    # ç»Ÿè®¡è·¨åˆ†ç±»å•†å“æ•°ï¼ˆç”¨äºæ—¥å¿—æ ¡éªŒï¼‰
    total_cross_cat = work_cat[work_cat['is_cross_category']]['base_name'].nunique()
    print(f"ğŸ” è·¨åˆ†ç±»å»é‡ï¼šæ£€æµ‹åˆ° {total_cross_cat} ä¸ªå•†å“å‡ºç°åœ¨å¤šä¸ªåˆ†ç±»ä¸­ï¼Œå·²æŒ‰ä¸»åˆ†ç±»å½’ç±»é¿å…é‡å¤è®¡æ•°")
    
    # åŸºäºå»é‡åçš„æ•°æ®è®¡ç®—åˆ†ç±»SKUæ•°
    vc_cat = work_cat_dedup.groupby(['l1_category','base_name'])['variant_key'].nunique(dropna=True).reset_index(name='vc')
    vc_cat['sku_contrib'] = vc_cat['vc'].apply(lambda x: int(x) if (pd.notna(x) and int(x) > 0) else 1)
    cat_sku_series = vc_cat.groupby('l1_category')['sku_contrib'].sum()
    # æ–°å¢ï¼šåˆ†ç±»å†…å¤šè§„æ ¼SKUæ€»æ•°ï¼ˆä¸æ˜¯å”¯ä¸€å¤šè§„æ ¼SPUæ•°ï¼‰ï¼Œå®šä¹‰ä¸º âˆ‘vcï¼ˆvc>1ï¼‰
    multi_sku_series = vc_cat.loc[vc_cat['vc'] > 1].groupby('l1_category')['vc'].sum()
    # æ¢å¤ï¼šåˆ†ç±»å†…å¤šè§„æ ¼SPUæ•°ï¼ˆvc>1 çš„ base_name ä¸ªæ•°ï¼‰
    multi_spu_series = vc_cat.assign(is_multi=vc_cat['vc'] > 1).groupby('l1_category')['is_multi'].sum()
    # è¦†ç›–è€å£å¾„
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»skuæ•°'] = cat_sku_series
    # å†™å›ï¼šåˆ†ç±»æ€»SKUå£å¾„ä¸å¤šè§„æ ¼SKU/SPUæ•°
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»å¤šè§„æ ¼SKUæ•°'] = multi_sku_series
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»å¤šè§„æ ¼SKUæ•°'] = l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»å¤šè§„æ ¼SKUæ•°'].fillna(0)
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»å¤šè§„æ ¼SPUæ•°'] = multi_spu_series
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»å¤šè§„æ ¼SPUæ•°'] = l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»å¤šè§„æ ¼SPUæ•°'].fillna(0)
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»0åº“å­˜ç‡'] = l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»0åº“å­˜æ•°'] / l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»skuæ•°']
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»skuå æ¯”'] = (l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»skuæ•°'] / all_skus_count) if all_skus_count > 0 else 0
    dedup_l1_counts = deduplicated.groupby('l1_category')['product_name'].nunique()
    active_l1_counts = active.groupby('l1_category')['product_name'].nunique()
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»åŠ¨é”€skuæ•°'] = active_l1_counts
    # ç±»å†…åŠ¨é”€ç‡ï¼šåˆ†ç±»å†…åŠ¨é”€SKU / åˆ†ç±»å†…å»é‡SKU
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»å»é‡SKUæ•°(å£å¾„åŒåŠ¨é”€ç‡)'] = dedup_l1_counts
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»åŠ¨é”€ç‡(ç±»å†…)'] = (active_l1_counts / dedup_l1_counts).fillna(0)
    
    # æ´»åŠ¨SKUè®¡ç®—ï¼šä½¿ç”¨ä¸åŠ¨é”€SKUç›¸åŒçš„å»é‡å£å¾„
    # å…ˆä»å»é‡æ•°æ®ä¸­ç­›é€‰å‡ºæœ‰æŠ˜æ‰£çš„å•†å“ï¼Œå†æŒ‰åˆ†ç±»ç»Ÿè®¡
    
    # ğŸ”§ ä¸´æ—¶è°ƒè¯•ï¼šæµ‹è¯•ä¸åŒæŠ˜æ‰£é˜ˆå€¼
    thresholds_to_test = [0, 0.01, 0.05, 0.1, 0.2]  # 0%, 1%, 5%, 10%, 20%
    
    print(f"ğŸ” æ´»åŠ¨SKUè¯Šæ–­ä¿¡æ¯:")
    print(f"   å»é‡åæ€»å•†å“æ•°: {len(deduplicated)}")
    
    # åˆ†ææŠ˜æ‰£åˆ†å¸ƒ
    discount_stats = deduplicated['discount'].describe()
    print(f"   æŠ˜æ‰£åˆ†å¸ƒç»Ÿè®¡: min={discount_stats['min']:.3f}, max={discount_stats['max']:.3f}, mean={discount_stats['mean']:.3f}")
    
    # æµ‹è¯•ä¸åŒé˜ˆå€¼çš„ç»“æœ
    threshold_results = {}
    for threshold in thresholds_to_test:
        count = len(deduplicated[deduplicated['discount'] > threshold])
        threshold_results[f'>{threshold*100:.0f}%'] = count
        
    print(f"   ä¸åŒæŠ˜æ‰£é˜ˆå€¼å•†å“æ•°: {threshold_results}")
    
    # æ£€æŸ¥åŸä»·å’Œå”®ä»·çš„å…³ç³»
    same_price_count = len(deduplicated[deduplicated['original_price'] == deduplicated['price']])
    price_diff_count = len(deduplicated[deduplicated['original_price'] != deduplicated['price']])
    
    print(f"   åŸä»·=å”®ä»·çš„å•†å“æ•°: {same_price_count}")
    print(f"   åŸä»·â‰ å”®ä»·çš„å•†å“æ•°: {price_diff_count}")
    
    # å¦‚æœåŸä»·=å”®ä»·çš„å•†å“å¾ˆå¤šï¼Œç»™å‡ºè­¦å‘Š
    if same_price_count > len(deduplicated) * 0.8:
        print(f"   âš ï¸  è­¦å‘Š: {same_price_count/len(deduplicated)*100:.1f}% çš„å•†å“åŸä»·=å”®ä»·")
        print(f"   ğŸ’¡ å»ºè®®: æ£€æŸ¥Excelä¸­æ˜¯å¦æœ‰å…¶ä»–æ´»åŠ¨æ ‡è¯†å­—æ®µ")
        
    # ğŸ”§ æ´»åŠ¨å•†å“å®šä¹‰ï¼šæŠ˜æ‰£ç‡>=10% æ‰ç®—çœŸæ­£çš„ä¿ƒé”€æ´»åŠ¨
    # é˜ˆå€¼è¯´æ˜ï¼š
    # - 0.10 = 10%æŠ˜æ‰£ï¼ˆå¦‚åŸä»·10å…ƒï¼Œå”®ä»·9å…ƒï¼‰
    # - 0.20 = 20%æŠ˜æ‰£ï¼ˆå¦‚åŸä»·10å…ƒï¼Œå”®ä»·8å…ƒï¼‰
    # - ä½äº10%çš„ä»·æ ¼å·®å¼‚å¯èƒ½æ˜¯å®šä»·ç­–ç•¥ï¼Œä¸ç®—ä¿ƒé”€æ´»åŠ¨
    ACTIVITY_THRESHOLD = 0.10  # 10%æŠ˜æ‰£é˜ˆå€¼ï¼Œç¬¦åˆé›¶å”®è¡Œä¸šå¸¸è§ä¿ƒé”€å®šä¹‰
    
    deduplicated_with_discount = deduplicated[deduplicated['discount'] > ACTIVITY_THRESHOLD]
    print(f"   âœ… ä½¿ç”¨é˜ˆå€¼ >{ACTIVITY_THRESHOLD*100:.0f}% çš„æ´»åŠ¨å•†å“æ•°: {len(deduplicated_with_discount)}")
    print(f"   ğŸ“Š æ´»åŠ¨å•†å“å æ¯”: {len(deduplicated_with_discount)/len(deduplicated)*100:.1f}%")
    
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨skuæ•°'] = deduplicated_with_discount.groupby('l1_category')['product_name'].nunique()
    # æ´»åŠ¨å æ¯”ï¼ˆç±»å†…ï¼‰ï¼šæ´»åŠ¨SKU / åˆ†ç±»å†…å»é‡SKU
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨å»é‡SKUæ•°(å£å¾„åŒå æ¯”)'] = dedup_l1_counts
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨SKUå æ¯”(ç±»å†…)'] = (l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨skuæ•°'] / dedup_l1_counts).fillna(0)
    
    # çˆ†å“SKUå’ŒæŠ˜æ‰£SKUä¹Ÿä½¿ç”¨ç›¸åŒçš„å»é‡å£å¾„å’Œä¸€è‡´çš„é˜ˆå€¼
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»çˆ†å“skuæ•°'] = deduplicated[deduplicated['discount'] > 0.701].groupby('l1_category')['product_name'].nunique()
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»æŠ˜æ‰£skuæ•°'] = deduplicated[deduplicated['discount'] > ACTIVITY_THRESHOLD].groupby('l1_category')['product_name'].nunique()
    # æœˆå”®ã€åŸä»·é”€å”®é¢ã€å”®ä»·é”€å”®é¢å‡æ”¹ä¸ºâ€œåˆ†ç±»å†…SPUå£å¾„å»é‡â€
    # å…ˆæ„é€  base_name
    work_ms = all_skus.copy()
    work_ms['base_name'] = work_ms['product_name'].apply(_normalize_base_name)
    # å¯¹æ¯ä¸ªSPUï¼Œå–æœ€ä½³ä»£è¡¨è§„æ ¼çš„åŸä»·/å”®ä»·é”€å”®é¢ï¼ˆå¤šçº§æ’åºï¼šé”€é‡ã€ä»·æ ¼ã€åº“å­˜ã€è§„æ ¼åï¼‰
    # å…ˆè¿›è¡Œå¤šçº§æ’åºï¼Œå†å–æ¯ç»„ç¬¬ä¸€è¡Œ
    work_ms_sorted = work_ms.sort_values(
        by=['sales_qty', 'price', 'åº“å­˜', 'è§„æ ¼åç§°'], 
        ascending=[False, True, False, True],
        na_position='last'
    )
    idx = work_ms_sorted.groupby(['l1_category','base_name']).head(1).index
    spu_ms = work_ms.loc[idx, ['l1_category','base_name','sales_qty','original_price_revenue','revenue']].copy()
    spu_ms = spu_ms.rename(columns={'sales_qty':'spuæœˆå”®','original_price_revenue':'spuåŸä»·é”€å”®é¢','revenue':'spuå”®ä»·é”€å”®é¢'})
    # æŒ‰ä¸€çº§åˆ†ç±»èšåˆ
    l1_month_sales_dedup = spu_ms.groupby('l1_category')['spuæœˆå”®'].sum()
    l1_sales_dedup = spu_ms.groupby('l1_category').agg(åŸä»·é”€å”®é¢=('spuåŸä»·é”€å”®é¢','sum'), å”®ä»·é”€å”®é¢=('spuå”®ä»·é”€å”®é¢','sum'))
    # åˆå¹¶å›åˆ†æè¡¨
    l1_analysis = l1_analysis.join(l1_sales_dedup, how='left')
    l1_analysis['æœˆå”®'] = l1_month_sales_dedup
    l1_analysis['æœˆå”®'] = pd.to_numeric(l1_analysis['æœˆå”®'], errors='coerce').fillna(0)
    # æœˆå”®å æ¯”åˆ†æ¯ä¹Ÿæ”¹ä¸ºå»é‡åçš„æ€»æœˆå”®
    total_month_sales_dedup = float(l1_month_sales_dedup.sum()) if hasattr(l1_month_sales_dedup, 'sum') else 0.0
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»æœˆå”®å æ¯”'] = (l1_analysis['æœˆå”®'] / total_month_sales_dedup) if total_month_sales_dedup > 0 else 0
    # æ ¡éªŒæ—¥å¿—ï¼šæœªå»é‡æ€»æœˆå”® vs SPUå£å¾„æ€»æœˆå”®
    try:
        raw_total_month = float(pd.to_numeric(all_skus['sales_qty'], errors='coerce').fillna(0).sum())
        print(f"ğŸ” æœˆå”®å£å¾„ | æœªå»é‡æ€»æœˆå”®={raw_total_month:.0f} | åˆ†ç±»å†…SPUå»é‡æ€»æœˆå”®={total_month_sales_dedup:.0f}")
    except Exception:
        pass
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»åŸä»·é”€å”®é¢å æ¯”'] = l1_analysis['åŸä»·é”€å”®é¢'] / all_skus['original_price_revenue'].sum() if all_skus['original_price_revenue'].sum() > 0 else 0
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»åŸä»·é”€å”®ä»¶å•ä»·'] = (l1_analysis['åŸä»·é”€å”®é¢'] / l1_analysis['æœˆå”®']).replace([np.inf, -np.inf], 0).fillna(0)
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»å”®ä»·é”€å”®é¢å æ¯”'] = l1_analysis['å”®ä»·é”€å”®é¢'] / all_skus['revenue'].sum() if all_skus['revenue'].sum() > 0 else 0
    # å°†â€œæŠ˜æ‰£ç‡(ç™¾åˆ†æ¯”)â€æ”¹ä¸ºâ€œæŠ˜æ‰£(æŠ˜)â€å±•ç¤ºï¼šåŠ æƒæŠ˜æ‰£ = å”®ä»·é”€å”®é¢ / åŸä»·é”€å”®é¢ï¼Œç„¶åä¹˜ä»¥10å¾—åˆ° x.x æŠ˜
    _ratio = (l1_analysis['å”®ä»·é”€å”®é¢'] / l1_analysis['åŸä»·é”€å”®é¢']).replace([np.inf, -np.inf], 0).fillna(0)
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»æŠ˜æ‰£'] = (_ratio * 10.0).clip(lower=0, upper=10)
    
    # ====== åˆ†ç±»æˆæœ¬èšåˆ ======
    if has_cost_data:
        # ä½¿ç”¨SPUå»é‡åçš„æˆæœ¬é”€å”®é¢å’Œæ¯›åˆ©æ±‡æ€»
        spu_ms['spuæˆæœ¬é”€å”®é¢'] = work_ms.loc[idx, 'æˆæœ¬é”€å”®é¢'].values
        spu_ms['spuæ¯›åˆ©'] = work_ms.loc[idx, 'æ¯›åˆ©'].values
        spu_ms['spuå®šä»·æ¯›åˆ©'] = work_ms.loc[idx, 'å®šä»·æ¯›åˆ©'].values
        
        l1_cost_agg = spu_ms.groupby('l1_category').agg(
            æˆæœ¬é”€å”®é¢=('spuæˆæœ¬é”€å”®é¢', 'sum'),
            æ¯›åˆ©=('spuæ¯›åˆ©', 'sum'),
            å®šä»·æ¯›åˆ©=('spuå®šä»·æ¯›åˆ©', 'sum')
        )
        
        l1_analysis = l1_analysis.join(l1_cost_agg, how='left')
        
        # å”®ä»·æ¯›åˆ©ç‡ï¼šæ¯›åˆ© / å”®ä»·é”€å”®é¢ï¼ˆå®é™…é”€å”®æƒ…å†µï¼‰
        l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»å”®ä»·æ¯›åˆ©ç‡'] = l1_analysis.apply(
            lambda row: (row['æ¯›åˆ©'] / row['å”®ä»·é”€å”®é¢']) if row['å”®ä»·é”€å”®é¢'] > 0 else 0,
            axis=1
        )
        
        # å®šä»·æ¯›åˆ©ç‡ï¼šå®šä»·æ¯›åˆ© / åŸä»·é”€å”®é¢ï¼ˆæŒ‰åŸä»·è®¡ç®—ï¼‰
        l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»å®šä»·æ¯›åˆ©ç‡'] = l1_analysis.apply(
            lambda row: (row['å®šä»·æ¯›åˆ©'] / row['åŸä»·é”€å”®é¢']) if row['åŸä»·é”€å”®é¢'] > 0 else 0,
            axis=1
        )
        
        # ä¿ç•™æ—§çš„"æ¯›åˆ©ç‡"åˆ—ä»¥å…¼å®¹ç°æœ‰ä»£ç ï¼ˆæŒ‡å‘å”®ä»·æ¯›åˆ©ç‡ï¼‰
        l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»æ¯›åˆ©ç‡'] = l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»å”®ä»·æ¯›åˆ©ç‡']
        
        # åˆ†ç±»æ¯›åˆ©è´¡çŒ®åº¦ï¼šæœ¬åˆ†ç±»æ¯›åˆ© / æ€»æ¯›åˆ©
        total_profit = l1_analysis['æ¯›åˆ©'].sum()
        l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»æ¯›åˆ©è´¡çŒ®åº¦'] = l1_analysis.apply(
            lambda row: (row['æ¯›åˆ©'] / total_profit) if total_profit > 0 else 0,
            axis=1
        )
        
        print(f"âœ… åˆ†ç±»æˆæœ¬èšåˆå®Œæˆï¼šæ€»æ¯›åˆ© Â¥{total_profit:,.2f}")
    
    # è·¨ç±»åŠ¨é”€å æ¯”ï¼šè¯¥åˆ†ç±»åŠ¨é”€SKU / å…¨éƒ¨åˆ†ç±»åŠ¨é”€SKU
    total_active = float(active_l1_counts.sum()) if hasattr(active_l1_counts, 'sum') else 0.0
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»åŠ¨é”€SKUå æ¯”(è·¨ç±»)'] = (active_l1_counts / total_active).fillna(0) if total_active > 0 else 0

    # è·¨ç±»æ´»åŠ¨å æ¯”ï¼šè¯¥åˆ†ç±»æ´»åŠ¨SKU / å…¨éƒ¨åˆ†ç±»æ´»åŠ¨SKU
    total_campaign = float(l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨skuæ•°'].sum()) if 'ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨skuæ•°' in l1_analysis.columns else 0.0
    l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨SKUå æ¯”(è·¨ç±»)'] = (l1_analysis['ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨skuæ•°'] / total_campaign).fillna(0) if total_campaign > 0 else 0

    # é‡æ’åˆ—é¡ºåºï¼šå°†åŠ¨é”€/æ´»åŠ¨çš„è®¡æ•°ä¸å æ¯”è´´è¿‘æ‘†æ”¾ï¼Œæå‡å¯è¯»æ€§
    try:
        preferred_order = [
            'ç¾å›¢ä¸€çº§åˆ†ç±»skuæ•°',
            'ç¾å›¢ä¸€çº§åˆ†ç±»å¤šè§„æ ¼SKUæ•°',
            'ç¾å›¢ä¸€çº§åˆ†ç±»å¤šè§„æ ¼SPUæ•°',
            'ç¾å›¢ä¸€çº§åˆ†ç±»å»é‡SKUæ•°(å£å¾„åŒåŠ¨é”€ç‡)',
            'ç¾å›¢ä¸€çº§åˆ†ç±»åŠ¨é”€skuæ•°',
            'ç¾å›¢ä¸€çº§åˆ†ç±»åŠ¨é”€ç‡(ç±»å†…)',
            'ç¾å›¢ä¸€çº§åˆ†ç±»åŠ¨é”€SKUå æ¯”(è·¨ç±»)',
            'ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨å»é‡SKUæ•°(å£å¾„åŒå æ¯”)',
            'ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨skuæ•°',
            'ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨SKUå æ¯”(ç±»å†…)',
            'ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨SKUå æ¯”(è·¨ç±»)',
            'ç¾å›¢ä¸€çº§åˆ†ç±»0åº“å­˜æ•°',
            'ç¾å›¢ä¸€çº§åˆ†ç±»0åº“å­˜ç‡',
            'ç¾å›¢ä¸€çº§åˆ†ç±»skuå æ¯”',
            'æœˆå”®',
            'ç¾å›¢ä¸€çº§åˆ†ç±»æœˆå”®å æ¯”',
            'åŸä»·é”€å”®é¢',
            'å”®ä»·é”€å”®é¢',
            'æˆæœ¬é”€å”®é¢',  # æ–°å¢
            'æ¯›åˆ©',  # æ–°å¢
            'ç¾å›¢ä¸€çº§åˆ†ç±»åŸä»·é”€å”®é¢å æ¯”',
            'ç¾å›¢ä¸€çº§åˆ†ç±»å”®ä»·é”€å”®é¢å æ¯”',
            'ç¾å›¢ä¸€çº§åˆ†ç±»æ¯›åˆ©ç‡',  # æ–°å¢
            'ç¾å›¢ä¸€çº§åˆ†ç±»æ¯›åˆ©è´¡çŒ®åº¦',  # æ–°å¢
            'ç¾å›¢ä¸€çº§åˆ†ç±»åŸä»·é”€å”®ä»¶å•ä»·',
            'ç¾å›¢ä¸€çº§åˆ†ç±»æŠ˜æ‰£skuæ•°',
            'ç¾å›¢ä¸€çº§åˆ†ç±»çˆ†å“skuæ•°',
            'ç¾å›¢ä¸€çº§åˆ†ç±»æŠ˜æ‰£',
        ]
        existing_pref = [c for c in preferred_order if c in l1_analysis.columns]
        remainder = [c for c in l1_analysis.columns if c not in existing_pref]
        l1_analysis = l1_analysis[existing_pref + remainder]
    except Exception:
        pass

    analysis_suite['ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡'] = l1_analysis.fillna(0).reset_index()
    
    # === ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡åˆ†æ ===
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‰çº§åˆ†ç±»æ•°æ®
    if 'l3_category' in all_skus.columns and not all_skus['l3_category'].isna().all():
        print(f"â„¹ï¸ å¼€å§‹è®¡ç®—ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡...")
        
        # å…ˆæŒ‰æ—§æ–¹å¼èšåˆ0åº“å­˜æ•°
        l3_analysis = all_skus.groupby('l3_category').agg(ç¾å›¢ä¸‰çº§åˆ†ç±»skuæ•°=('product_name', 'size'), ç¾å›¢ä¸‰çº§åˆ†ç±»0åº“å­˜æ•°=('åº“å­˜', lambda x: (x == 0).sum()))
        
        # ğŸ”§ æ–¹æ¡ˆAï¼šè·¨åˆ†ç±»å»é‡é€»è¾‘ï¼ˆä¸æ ¸å¿ƒæŒ‡æ ‡ä¿æŒä¸€è‡´ï¼‰
        work_cat_l3 = all_skus.copy()
        work_cat_l3['base_name'] = work_cat_l3['product_name'].apply(_normalize_base_name)
        work_cat_l3['variant_key'] = work_cat_l3.apply(_vk_cat, axis=1)
        
        # ä¸ºæ¯ä¸ª base_name æ ‡è®°ä¸»åˆ†ç±»ï¼ˆé¦–æ¬¡å‡ºç°çš„ä¸‰çº§åˆ†ç±»ï¼‰
        work_cat_l3['primary_category_l3'] = work_cat_l3.groupby('base_name')['l3_category'].transform('first')
        
        # åªä¿ç•™ä¸»åˆ†ç±»çš„è®°å½•è¿›è¡Œç»Ÿè®¡ï¼ˆé¿å…è·¨åˆ†ç±»é‡å¤è®¡æ•°ï¼‰
        work_cat_l3_dedup = work_cat_l3[work_cat_l3['l3_category'] == work_cat_l3['primary_category_l3']].copy()
        
        vc_cat_l3 = work_cat_l3_dedup.groupby(['l3_category','base_name'])['variant_key'].nunique(dropna=True).reset_index(name='vc')
        vc_cat_l3['sku_contrib'] = vc_cat_l3['vc'].apply(lambda x: int(x) if (pd.notna(x) and int(x) > 0) else 1)
        cat_sku_series_l3 = vc_cat_l3.groupby('l3_category')['sku_contrib'].sum()
        
        # åˆ†ç±»å†…å¤šè§„æ ¼SKUæ€»æ•°å’Œå¤šè§„æ ¼SPUæ•°
        multi_sku_series_l3 = vc_cat_l3.loc[vc_cat_l3['vc'] > 1].groupby('l3_category')['vc'].sum()
        multi_spu_series_l3 = vc_cat_l3.assign(is_multi=vc_cat_l3['vc'] > 1).groupby('l3_category')['is_multi'].sum()
        
        # è¦†ç›–è€å£å¾„
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»skuæ•°'] = cat_sku_series_l3
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»å¤šè§„æ ¼SKUæ•°'] = multi_sku_series_l3.fillna(0)
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»å¤šè§„æ ¼SPUæ•°'] = multi_spu_series_l3.fillna(0)
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»0åº“å­˜ç‡'] = l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»0åº“å­˜æ•°'] / l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»skuæ•°']
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»skuå æ¯”'] = (l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»skuæ•°'] / all_skus_count) if all_skus_count > 0 else 0
        
        dedup_l3_counts = deduplicated.groupby('l3_category')['product_name'].nunique()
        active_l3_counts = active.groupby('l3_category')['product_name'].nunique()
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»åŠ¨é”€skuæ•°'] = active_l3_counts
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»å»é‡SKUæ•°(å£å¾„åŒåŠ¨é”€ç‡)'] = dedup_l3_counts
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»åŠ¨é”€ç‡(ç±»å†…)'] = (active_l3_counts / dedup_l3_counts).fillna(0)
        
        # æ´»åŠ¨SKUè®¡ç®—ï¼šä½¿ç”¨ä¸ä¸€çº§åˆ†ç±»ç›¸åŒçš„é˜ˆå€¼å’Œé€»è¾‘
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨skuæ•°'] = deduplicated_with_discount.groupby('l3_category')['product_name'].nunique()
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨å»é‡SKUæ•°(å£å¾„åŒå æ¯”)'] = dedup_l3_counts
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨SKUå æ¯”(ç±»å†…)'] = (l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨skuæ•°'] / dedup_l3_counts).fillna(0)
        
        # çˆ†å“SKUå’ŒæŠ˜æ‰£SKU
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»çˆ†å“skuæ•°'] = deduplicated[deduplicated['discount'] > 0.701].groupby('l3_category')['product_name'].nunique()
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»æŠ˜æ‰£skuæ•°'] = deduplicated[deduplicated['discount'] > ACTIVITY_THRESHOLD].groupby('l3_category')['product_name'].nunique()
        
        # æœˆå”®ã€åŸä»·é”€å”®é¢ã€å”®ä»·é”€å”®é¢ï¼ˆSPUå£å¾„å»é‡ï¼‰
        work_ms_l3 = all_skus.copy()
        work_ms_l3['base_name'] = work_ms_l3['product_name'].apply(_normalize_base_name)
        work_ms_sorted_l3 = work_ms_l3.sort_values(
            by=['sales_qty', 'price', 'åº“å­˜', 'è§„æ ¼åç§°'], 
            ascending=[False, True, False, True],
            na_position='last'
        )
        idx_l3 = work_ms_sorted_l3.groupby(['l3_category','base_name']).head(1).index
        spu_ms_l3 = work_ms_l3.loc[idx_l3, ['l3_category','base_name','sales_qty','original_price_revenue','revenue']].copy()
        spu_ms_l3 = spu_ms_l3.rename(columns={'sales_qty':'spuæœˆå”®','original_price_revenue':'spuåŸä»·é”€å”®é¢','revenue':'spuå”®ä»·é”€å”®é¢'})
        
        # æŒ‰ä¸‰çº§åˆ†ç±»èšåˆ
        l3_month_sales_dedup = spu_ms_l3.groupby('l3_category')['spuæœˆå”®'].sum()
        l3_sales_dedup = spu_ms_l3.groupby('l3_category').agg(åŸä»·é”€å”®é¢=('spuåŸä»·é”€å”®é¢','sum'), å”®ä»·é”€å”®é¢=('spuå”®ä»·é”€å”®é¢','sum'))
        
        # åˆå¹¶å›åˆ†æè¡¨
        l3_analysis = l3_analysis.join(l3_sales_dedup, how='left')
        l3_analysis['æœˆå”®'] = l3_month_sales_dedup
        l3_analysis['æœˆå”®'] = pd.to_numeric(l3_analysis['æœˆå”®'], errors='coerce').fillna(0)
        
        # æœˆå”®å æ¯”
        total_month_sales_dedup_l3 = float(l3_month_sales_dedup.sum()) if hasattr(l3_month_sales_dedup, 'sum') else 0.0
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»æœˆå”®å æ¯”'] = (l3_analysis['æœˆå”®'] / total_month_sales_dedup_l3) if total_month_sales_dedup_l3 > 0 else 0
        
        # é”€å”®é¢å æ¯”å’Œä»¶å•ä»·
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»åŸä»·é”€å”®é¢å æ¯”'] = l3_analysis['åŸä»·é”€å”®é¢'] / all_skus['original_price_revenue'].sum() if all_skus['original_price_revenue'].sum() > 0 else 0
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»åŸä»·é”€å”®ä»¶å•ä»·'] = (l3_analysis['åŸä»·é”€å”®é¢'] / l3_analysis['æœˆå”®']).replace([np.inf, -np.inf], 0).fillna(0)
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»å”®ä»·é”€å”®é¢å æ¯”'] = l3_analysis['å”®ä»·é”€å”®é¢'] / all_skus['revenue'].sum() if all_skus['revenue'].sum() > 0 else 0
        
        # æŠ˜æ‰£ï¼ˆæŠ˜ï¼‰å±•ç¤º
        _ratio_l3 = (l3_analysis['å”®ä»·é”€å”®é¢'] / l3_analysis['åŸä»·é”€å”®é¢']).replace([np.inf, -np.inf], 0).fillna(0)
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»æŠ˜æ‰£'] = (_ratio_l3 * 10.0).clip(lower=0, upper=10)
        
        # è·¨ç±»åŠ¨é”€å æ¯”å’Œæ´»åŠ¨å æ¯”
        total_active_l3 = float(active_l3_counts.sum()) if hasattr(active_l3_counts, 'sum') else 0.0
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»åŠ¨é”€SKUå æ¯”(è·¨ç±»)'] = (active_l3_counts / total_active_l3).fillna(0) if total_active_l3 > 0 else 0
        
        total_campaign_l3 = float(l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨skuæ•°'].sum()) if 'ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨skuæ•°' in l3_analysis.columns else 0.0
        l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨SKUå æ¯”(è·¨ç±»)'] = (l3_analysis['ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨skuæ•°'] / total_campaign_l3).fillna(0) if total_campaign_l3 > 0 else 0
        
        # é‡æ’åˆ—é¡ºåºï¼ˆä¸ä¸€çº§åˆ†ç±»ä¿æŒä¸€è‡´çš„é€»è¾‘ï¼‰
        try:
            preferred_order_l3 = [
                'ç¾å›¢ä¸‰çº§åˆ†ç±»skuæ•°',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»å¤šè§„æ ¼SKUæ•°',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»å¤šè§„æ ¼SPUæ•°',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»å»é‡SKUæ•°(å£å¾„åŒåŠ¨é”€ç‡)',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»åŠ¨é”€skuæ•°',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»åŠ¨é”€ç‡(ç±»å†…)',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»åŠ¨é”€SKUå æ¯”(è·¨ç±»)',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨å»é‡SKUæ•°(å£å¾„åŒå æ¯”)',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨skuæ•°',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨SKUå æ¯”(ç±»å†…)',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨SKUå æ¯”(è·¨ç±»)',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»0åº“å­˜æ•°',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»0åº“å­˜ç‡',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»skuå æ¯”',
                'æœˆå”®',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»æœˆå”®å æ¯”',
                'åŸä»·é”€å”®é¢',
                'å”®ä»·é”€å”®é¢',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»åŸä»·é”€å”®é¢å æ¯”',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»å”®ä»·é”€å”®é¢å æ¯”',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»åŸä»·é”€å”®ä»¶å•ä»·',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»æŠ˜æ‰£skuæ•°',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»çˆ†å“skuæ•°',
                'ç¾å›¢ä¸‰çº§åˆ†ç±»æŠ˜æ‰£',
            ]
            existing_pref_l3 = [c for c in preferred_order_l3 if c in l3_analysis.columns]
            remainder_l3 = [c for c in l3_analysis.columns if c not in existing_pref_l3]
            l3_analysis = l3_analysis[existing_pref_l3 + remainder_l3]
        except Exception:
            pass

        analysis_suite['ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡'] = l3_analysis.fillna(0).reset_index()
        print(f"âœ… ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡åˆ†æå®Œæˆï¼Œå…± {len(l3_analysis)} ä¸ªä¸‰çº§åˆ†ç±»ã€‚")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ä¸‰çº§åˆ†ç±»æ•°æ®ï¼Œè·³è¿‡ä¸‰çº§åˆ†ç±»åˆ†æã€‚")
    
    # ====== æˆæœ¬åˆ†ææ±‡æ€»Sheet ======
    if has_cost_data and 'l1_category' in all_skus.columns:
        print(f"â„¹ï¸ å¼€å§‹ç”Ÿæˆæˆæœ¬åˆ†ææ±‡æ€»...")
        
        cost_summary_df = l1_analysis[[
            'æˆæœ¬é”€å”®é¢', 'å”®ä»·é”€å”®é¢', 'åŸä»·é”€å”®é¢', 'æ¯›åˆ©', 'å®šä»·æ¯›åˆ©',
            'ç¾å›¢ä¸€çº§åˆ†ç±»å”®ä»·æ¯›åˆ©ç‡', 'ç¾å›¢ä¸€çº§åˆ†ç±»å®šä»·æ¯›åˆ©ç‡', 'ç¾å›¢ä¸€çº§åˆ†ç±»æ¯›åˆ©è´¡çŒ®åº¦'
        ]].copy() if has_cost_data else pd.DataFrame()
        
        if not cost_summary_df.empty:
            # æ·»åŠ åˆ†ç±»åç§°ä½œä¸ºåˆ—
            cost_summary_df.insert(0, 'ç¾å›¢ä¸€çº§åˆ†ç±»', cost_summary_df.index)
            
            # æ’åºï¼šæŒ‰å”®ä»·æ¯›åˆ©ç‡é™åº
            cost_summary_df = cost_summary_df.sort_values('ç¾å›¢ä¸€çº§åˆ†ç±»å”®ä»·æ¯›åˆ©ç‡', ascending=False)
            
            # æ·»åŠ é«˜ä½æ¯›åˆ©å•†å“ç»Ÿè®¡
            high_margin_count = (all_skus['å”®ä»·æ¯›åˆ©ç‡'] >= 0.5).sum()
            low_margin_count = (all_skus['å”®ä»·æ¯›åˆ©ç‡'] < 0.1).sum()
            
            # åœ¨Sheetä¸­æ·»åŠ æ±‡æ€»è¡Œ
            total_row = pd.DataFrame({
                'ç¾å›¢ä¸€çº§åˆ†ç±»': ['å…¨éƒ¨åˆ†ç±»æ±‡æ€»'],
                'æˆæœ¬é”€å”®é¢': [cost_summary_df['æˆæœ¬é”€å”®é¢'].sum()],
                'å”®ä»·é”€å”®é¢': [cost_summary_df['å”®ä»·é”€å”®é¢'].sum()],
                'åŸä»·é”€å”®é¢': [cost_summary_df['åŸä»·é”€å”®é¢'].sum()],
                'æ¯›åˆ©': [cost_summary_df['æ¯›åˆ©'].sum()],
                'å®šä»·æ¯›åˆ©': [cost_summary_df['å®šä»·æ¯›åˆ©'].sum()],
                'ç¾å›¢ä¸€çº§åˆ†ç±»å”®ä»·æ¯›åˆ©ç‡': [cost_summary_df['æ¯›åˆ©'].sum() / cost_summary_df['å”®ä»·é”€å”®é¢'].sum() if cost_summary_df['å”®ä»·é”€å”®é¢'].sum() > 0 else 0],
                'ç¾å›¢ä¸€çº§åˆ†ç±»å®šä»·æ¯›åˆ©ç‡': [cost_summary_df['å®šä»·æ¯›åˆ©'].sum() / cost_summary_df['åŸä»·é”€å”®é¢'].sum() if cost_summary_df['åŸä»·é”€å”®é¢'].sum() > 0 else 0],
                'ç¾å›¢ä¸€çº§åˆ†ç±»æ¯›åˆ©è´¡çŒ®åº¦': [1.0]
            })
            
            cost_summary_df = pd.concat([total_row, cost_summary_df], ignore_index=True)
            
            analysis_suite['æˆæœ¬åˆ†ææ±‡æ€»'] = cost_summary_df
            
            # é«˜æ¯›åˆ©å•†å“TOP50ï¼ˆæŒ‰å”®ä»·æ¯›åˆ©ç‡ç­›é€‰ï¼‰
            high_margin_skus = all_skus[all_skus['å”®ä»·æ¯›åˆ©ç‡'] >= 0.3].copy()
            if not high_margin_skus.empty:
                high_margin_skus = high_margin_skus.sort_values('æ¯›åˆ©', ascending=False).head(50)
                high_margin_top50 = high_margin_skus[[
                    'product_name', 'l1_category', 'price', 'original_price', 'cost', 
                    'æ¯›åˆ©', 'å”®ä»·æ¯›åˆ©ç‡', 'å®šä»·æ¯›åˆ©ç‡', 
                    'sales_qty', 'revenue', 'æˆæœ¬é”€å”®é¢'
                ]].copy()
                # é‡å‘½ååˆ—ä¾¿äºç†è§£
                high_margin_top50 = high_margin_top50.rename(columns={
                    'price': 'å”®ä»·',
                    'original_price': 'åŸä»·',
                    'sales_qty': 'æœˆå”®',
                    'revenue': 'å”®ä»·é”€å”®é¢'
                })
                analysis_suite['é«˜æ¯›åˆ©å•†å“TOP50'] = high_margin_top50
            
            # ä½æ¯›åˆ©é¢„è­¦å•†å“ï¼ˆæŒ‰å”®ä»·æ¯›åˆ©ç‡ç­›é€‰ï¼‰
            low_margin_skus = all_skus[all_skus['å”®ä»·æ¯›åˆ©ç‡'] < 0.1].copy()
            if not low_margin_skus.empty:
                low_margin_skus = low_margin_skus.sort_values('revenue', ascending=False).head(100)
                low_margin_warning = low_margin_skus[[
                    'product_name', 'l1_category', 'price', 'original_price', 'cost', 
                    'æ¯›åˆ©', 'å”®ä»·æ¯›åˆ©ç‡', 'å®šä»·æ¯›åˆ©ç‡',
                    'sales_qty', 'revenue', 'æˆæœ¬é”€å”®é¢'
                ]].copy()
                # é‡å‘½ååˆ—ä¾¿äºç†è§£
                low_margin_warning = low_margin_warning.rename(columns={
                    'price': 'å”®ä»·',
                    'original_price': 'åŸä»·',
                    'sales_qty': 'æœˆå”®',
                    'revenue': 'å”®ä»·é”€å”®é¢'
                })
                analysis_suite['ä½æ¯›åˆ©é¢„è­¦å•†å“'] = low_margin_warning
            
            print(f"âœ… æˆæœ¬åˆ†ææ±‡æ€»å®Œæˆï¼šé«˜æ¯›åˆ©å•†å“{len(high_margin_skus)}ä¸ªï¼Œä½æ¯›åˆ©å•†å“{len(low_margin_skus)}ä¸ª")
    
    return analysis_suite

def export_full_report_to_excel(all_results, all_store_data, output_filename):
    """å°†æ‰€æœ‰åˆ†æç»“æœå’Œè¯¦ç»†æŠ¥å‘Šå¯¼å‡ºåˆ°Excelã€‚"""
    # è§„èŒƒåŒ–è¾“å‡ºè·¯å¾„ä¸å ç”¨å¤„ç†
    output_path = Path(output_filename).resolve()
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # è‹¥åŒåæ–‡ä»¶å·²å­˜åœ¨ï¼Œå°è¯•åˆ é™¤ï¼›è‹¥è¢«å ç”¨ï¼Œåˆ™æ”¹ä¸ºæ—¶é—´æˆ³æ–‡ä»¶å
    if output_path.exists():
        try:
            os.remove(output_path)
        except Exception as e:
            ts = dt.datetime.now().strftime('%Y%m%d_%H%M%S')
            alt = output_path.with_name(f"{output_path.stem}_{ts}{output_path.suffix}")
            print(f"âš ï¸ ç›®æ ‡æ–‡ä»¶è¢«å ç”¨æˆ–æ— æ³•è¦†ç›–ï¼ˆ{e}ï¼‰ã€‚å°†æ”¹ä¸ºè¾“å‡º: {alt.name}")
            output_path = alt

    print(f"\nâ³ æ­£åœ¨ç”ŸæˆExcelæŠ¥å‘Š: {output_path.name}...")

    # ä¼˜å…ˆä½¿ç”¨ xlsxwriterï¼Œä¸å¯ç”¨æ—¶å›é€€ openpyxlï¼›å¦‚ä»å¤±è´¥ï¼Œé™„å¸¦æ—¶é—´æˆ³é‡è¯•ä¸€æ¬¡
    engine_name = 'xlsxwriter'
    try:
        writer = pd.ExcelWriter(str(output_path), engine='xlsxwriter')
    except Exception:
        print("âš ï¸ xlsxwriter ä¸å¯ç”¨ï¼Œå›é€€åˆ° openpyxlã€‚å»ºè®®: pip install XlsxWriter ä»¥è·å¾—æ›´ä½³å…¼å®¹ä¸æ ¼å¼æ”¯æŒã€‚")
        engine_name = 'openpyxl'
        try:
            writer = pd.ExcelWriter(str(output_path), engine='openpyxl')
        except PermissionError as e:
            ts = dt.datetime.now().strftime('%Y%m%d_%H%M%S')
            alt = output_path.with_name(f"{output_path.stem}_{ts}{output_path.suffix}")
            print(f"âš ï¸ æ‰“å¼€æ–‡ä»¶å¤±è´¥ï¼ˆ{e}ï¼‰ã€‚å¯èƒ½è¢«å ç”¨ï¼Œå°†æ”¹ä¸ºè¾“å‡º: {alt.name}")
            writer = pd.ExcelWriter(str(alt), engine='openpyxl')
    with writer:
        # æŒ‰ Sheet ç™½åå•è¯†åˆ«ç™¾åˆ†æ¯”åˆ—ï¼›è‹¥ç™½åå•ç¼ºå¤±åˆ™å›é€€åˆ°æ•°å€¼èŒƒå›´(0..1)åˆ¤æ–­
        def get_sheet_pct_cols(sheet_name, df):
            whitelist = {
                'å•†å“è§’è‰²åˆ†æ': ['é”€å”®é¢å æ¯”', 'SKUå æ¯”'],
                'ä»·æ ¼å¸¦åˆ†æ': ['é”€å”®é¢å æ¯”', 'SKUå æ¯”'],
                'ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡': [
                    'ç¾å›¢ä¸€çº§åˆ†ç±»0åº“å­˜ç‡',
                    'ç¾å›¢ä¸€çº§åˆ†ç±»skuå æ¯”',
                    'ç¾å›¢ä¸€çº§åˆ†ç±»åŠ¨é”€ç‡(ç±»å†…)',
                    'ç¾å›¢ä¸€çº§åˆ†ç±»åŠ¨é”€SKUå æ¯”(è·¨ç±»)',
                    'ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨SKUå æ¯”(ç±»å†…)',
                    'ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨SKUå æ¯”(è·¨ç±»)',
                    'ç¾å›¢ä¸€çº§åˆ†ç±»æœˆå”®å æ¯”',
                    'ç¾å›¢ä¸€çº§åˆ†ç±»åŸä»·é”€å”®é¢å æ¯”',
                    'ç¾å›¢ä¸€çº§åˆ†ç±»å”®ä»·é”€å”®é¢å æ¯”',
                    'ç¾å›¢ä¸€çº§åˆ†ç±»å”®ä»·æ¯›åˆ©ç‡',  # æ–°å¢
                    'ç¾å›¢ä¸€çº§åˆ†ç±»å®šä»·æ¯›åˆ©ç‡',  # æ–°å¢
                    'ç¾å›¢ä¸€çº§åˆ†ç±»æ¯›åˆ©ç‡',  # å…¼å®¹æ—§ä»£ç 
                    'ç¾å›¢ä¸€çº§åˆ†ç±»æ¯›åˆ©è´¡çŒ®åº¦',  # æ–°å¢
                ],
                'ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡': [
                    'ç¾å›¢ä¸‰çº§åˆ†ç±»0åº“å­˜ç‡',
                    'ç¾å›¢ä¸‰çº§åˆ†ç±»skuå æ¯”',
                    'ç¾å›¢ä¸‰çº§åˆ†ç±»åŠ¨é”€ç‡(ç±»å†…)',
                    'ç¾å›¢ä¸‰çº§åˆ†ç±»åŠ¨é”€SKUå æ¯”(è·¨ç±»)',
                    'ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨SKUå æ¯”(ç±»å†…)',
                    'ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨SKUå æ¯”(è·¨ç±»)',
                    'ç¾å›¢ä¸‰çº§åˆ†ç±»æœˆå”®å æ¯”',
                    'ç¾å›¢ä¸‰çº§åˆ†ç±»åŸä»·é”€å”®é¢å æ¯”',
                    'ç¾å›¢ä¸‰çº§åˆ†ç±»å”®ä»·é”€å”®é¢å æ¯”',
                ],
                'æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”': ['åŠ¨é”€ç‡'],
                'æˆæœ¬åˆ†ææ±‡æ€»': ['ç¾å›¢ä¸€çº§åˆ†ç±»å”®ä»·æ¯›åˆ©ç‡', 'ç¾å›¢ä¸€çº§åˆ†ç±»å®šä»·æ¯›åˆ©ç‡', 'ç¾å›¢ä¸€çº§åˆ†ç±»æ¯›åˆ©ç‡', 'ç¾å›¢ä¸€çº§åˆ†ç±»æ¯›åˆ©è´¡çŒ®åº¦'],  # æ–°å¢
                'é«˜æ¯›åˆ©å•†å“TOP50': ['å”®ä»·æ¯›åˆ©ç‡', 'å®šä»·æ¯›åˆ©ç‡', 'æ¯›åˆ©ç‡'],  # æ–°å¢
                'ä½æ¯›åˆ©é¢„è­¦å•†å“': ['å”®ä»·æ¯›åˆ©ç‡', 'å®šä»·æ¯›åˆ©ç‡', 'æ¯›åˆ©ç‡'],  # æ–°å¢
            }
            wl = whitelist.get(sheet_name, [])
            pct_cols = [c for c in wl if c in df.columns]
            # å…œåº•ï¼šé€‰æ‹© dtype ä¸ºæµ®ç‚¹ï¼Œä¸”å€¼åŸŸåœ¨ 0..1 çš„åˆ—
            if not pct_cols:
                for c in df.columns:
                    if 'å•ä»·' in str(c):
                        continue
                    s = df[c]
                    if pd.api.types.is_float_dtype(s):
                        try:
                            s_non_null = s.dropna()
                            if not s_non_null.empty:
                                vmin = float(s_non_null.min())
                                vmax = float(s_non_null.max())
                                if vmin >= 0.0 and vmax <= 1.000001:
                                    pct_cols.append(c)
                        except Exception:
                            pass
            return pct_cols

        # xlsxwriterï¼šæŒ‰åˆ—ååˆ—è¡¨è®¾ç½® 0.00%ï¼ˆè€ƒè™‘ç´¢å¼•å±‚æ•°åç§»ï¼‰
        def apply_pct_xlsxwriter(ws, df, pct_cols, index_written=True):
            if ws is None or not pct_cols:
                return
            wb = writer.book
            fmt_pct = wb.add_format({'num_format': '0.00%'})
            offset = df.index.nlevels if index_written else 0
            for col_name in pct_cols:
                try:
                    col_idx = int(df.columns.get_loc(col_name)) + offset
                    ws.set_column(col_idx, col_idx, None, fmt_pct)
                except Exception:
                    pass

        # openpyxlï¼šæŒ‰åˆ—ååˆ—è¡¨è®¾ç½® 0.00%
        def apply_pct_openpyxl(ws, pct_cols):
            if ws is None or not pct_cols:
                return
            header_map = {}
            for cell in ws[1]:
                header_map[str(cell.value)] = cell.column
            max_row = ws.max_row
            for name in pct_cols:
                c = header_map.get(str(name))
                if c is None:
                    continue
                for r in range(2, max_row + 1):
                    ws.cell(row=r, column=c).number_format = '0.00%'

        # å†™å…¥å„ Sheet (ä¸­æ–‡åŒ–è¡¨å¤´)
        # å®šä¹‰åˆ—åä¸­æ–‡åŒ–æ˜ å°„
        column_cn_mapping = {
            # æ ¸å¿ƒæŒ‡æ ‡
            'æ€»SKUæ•°(å«è§„æ ¼)': 'æ€»SKUæ•°(å«è§„æ ¼)',
            'å•è§„æ ¼SPUæ•°': 'å•è§„æ ¼SPUæ•°', 
            'å•è§„æ ¼SKUæ•°': 'å•è§„æ ¼SKUæ•°',
            'å¤šè§„æ ¼SKUæ€»æ•°': 'å¤šè§„æ ¼SKUæ€»æ•°',
            'æ€»SKUæ•°(å»é‡å)': 'æ€»SKUæ•°(å»é‡å)',
            'åŠ¨é”€SKUæ•°': 'åŠ¨é”€SKUæ•°',
            'æ»é”€SKUæ•°': 'æ»é”€SKUæ•°', 
            'æ€»é”€å”®é¢(å»é‡å)': 'æ€»é”€å”®é¢(å»é‡å)',
            'åŠ¨é”€ç‡': 'åŠ¨é”€ç‡',
            'å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°': 'å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°',
            # è§’è‰²/ä»·æ ¼å¸¦åˆ†æ
            'SKUæ•°é‡': 'SKUæ•°é‡',
            'é”€å”®é¢': 'é”€å”®é¢',
            'é”€å”®é¢å æ¯”': 'é”€å”®é¢å æ¯”',
            'SKUå æ¯”': 'SKUå æ¯”',
            # å¤šè§„æ ¼ç›¸å…³ - æ‰©å±•æ˜ å°„
            'Store': 'é—¨åº—',
            'product_name': 'å•†å“åç§°', 
            'base_name': 'åŸºç¡€åç§°',
            'l1_category': 'ä¸€çº§åˆ†ç±»',
            'l3_category': 'ä¸‰çº§åˆ†ç±»',
            'è§„æ ¼ç§ç±»æ•°': 'è§„æ ¼ç§ç±»æ•°',
            'å¤šè§„æ ¼ä¾æ®': 'å¤šè§„æ ¼ä¾æ®',
            'sales_qty': 'æœˆå”®',
            # å…¶ä»–å¯èƒ½ç”¨åˆ°çš„åˆ—
            'price': 'å”®ä»·',
            'original_price': 'åŸä»·',
            'revenue': 'å”®ä»·é”€å”®é¢',
            'original_price_revenue': 'åŸä»·é”€å”®é¢',
            'price_band': 'ä»·æ ¼å¸¦',
            'role': 'å•†å“è§’è‰²',
            'discount': 'æŠ˜æ‰£',
            'åº“å­˜': 'åº“å­˜',
            'è§„æ ¼åç§°': 'è§„æ ¼åç§°',
            'barcode': 'æ¡ç ',
            'å•†å®¶åˆ†ç±»': 'å•†å®¶åˆ†ç±»',
            'variant_key': 'å˜ä½“é”®',
            'inferred_spec': 'æ¨æ–­è§„æ ¼'
        }
        
        # åº”ç”¨ä¸­æ–‡åˆ—åæ˜ å°„å‡½æ•°
        def apply_cn_columns(df):
            df_copy = df.copy()
            df_copy.columns = [column_cn_mapping.get(col, col) for col in df_copy.columns]
            return df_copy
        
        core_kpi_df = pd.concat([res['æ€»ä½“æŒ‡æ ‡'] for res in all_results.values() if 'æ€»ä½“æŒ‡æ ‡' in res])
        core_kpi_df = apply_cn_columns(core_kpi_df)
        core_kpi_df.to_excel(writer, sheet_name='æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”')

        role_df = pd.concat([res['å•†å“è§’è‰²åˆ†æ'] for res in all_results.values() if 'å•†å“è§’è‰²åˆ†æ' in res], keys=all_results.keys())
        role_df = apply_cn_columns(role_df)
        role_df.to_excel(writer, sheet_name='å•†å“è§’è‰²åˆ†æ')

        price_df = pd.concat([res['ä»·æ ¼å¸¦åˆ†æ'] for res in all_results.values() if 'ä»·æ ¼å¸¦åˆ†æ' in res], keys=all_results.keys())
        price_df = apply_cn_columns(price_df)
        price_df.to_excel(writer, sheet_name='ä»·æ ¼å¸¦åˆ†æ')

        # ç”Ÿæˆä¸€è‡´æ€§æ ¡éªŒè¡¨ï¼šè§’è‰²/ä»·æ ¼å¸¦çš„SKUä¸é”€å”®é¢æ±‡æ€»éœ€åˆ†åˆ«ç­‰äº åŠ¨é”€SKUæ•°/å»é‡æ€»é”€å”®é¢
        try:
            # æŒ‰é—¨åº—èšåˆä¸¤å¼ è¡¨
            role_agg = role_df.groupby(level=0).agg(è§’è‰²SKUæ±‡æ€»=('SKUæ•°é‡', 'sum'), è§’è‰²é”€å”®é¢æ±‡æ€»=('é”€å”®é¢', 'sum'))
            price_agg = price_df.groupby(level=0).agg(ä»·æ ¼å¸¦SKUæ±‡æ€»=('SKUæ•°é‡', 'sum'), ä»·æ ¼å¸¦é”€å”®é¢æ±‡æ€»=('é”€å”®é¢', 'sum'))
            # KPI åŸºå‡†
            kpi_base = core_kpi_df[['åŠ¨é”€SKUæ•°', 'æ€»é”€å”®é¢(å»é‡å)']].copy()
            # åˆå¹¶
            chk = kpi_base.join(role_agg, how='left').join(price_agg, how='left')
            # è®¡ç®—ä¸€è‡´æ€§å¸ƒå°”é¡¹ï¼ˆé”€å”®é¢å…è®¸å°‘é‡æµ®ç‚¹è¯¯å·®ï¼‰
            def _isclose(a, b):
                try:
                    return bool(np.isclose(float(a), float(b), rtol=1e-6, atol=0.01))
                except Exception:
                    return False
            chk['æ ¡éªŒ_è§’è‰²SKUä¸€è‡´'] = (pd.to_numeric(chk['è§’è‰²SKUæ±‡æ€»'], errors='coerce').fillna(0).astype(int) == pd.to_numeric(chk['åŠ¨é”€SKUæ•°'], errors='coerce').fillna(0).astype(int))
            chk['æ ¡éªŒ_ä»·æ ¼å¸¦SKUä¸€è‡´'] = (pd.to_numeric(chk['ä»·æ ¼å¸¦SKUæ±‡æ€»'], errors='coerce').fillna(0).astype(int) == pd.to_numeric(chk['åŠ¨é”€SKUæ•°'], errors='coerce').fillna(0).astype(int))
            chk['æ ¡éªŒ_è§’è‰²é”€å”®é¢ä¸€è‡´'] = [
                _isclose(a, b) for a, b in zip(pd.to_numeric(chk['è§’è‰²é”€å”®é¢æ±‡æ€»'], errors='coerce').fillna(0), pd.to_numeric(chk['æ€»é”€å”®é¢(å»é‡å)'], errors='coerce').fillna(0))
            ]
            chk['æ ¡éªŒ_ä»·æ ¼å¸¦é”€å”®é¢ä¸€è‡´'] = [
                _isclose(a, b) for a, b in zip(pd.to_numeric(chk['ä»·æ ¼å¸¦é”€å”®é¢æ±‡æ€»'], errors='coerce').fillna(0), pd.to_numeric(chk['æ€»é”€å”®é¢(å»é‡å)'], errors='coerce').fillna(0))
            ]
            # è¾“å‡º
            chk.index.name = 'é—¨åº—'
            # å‹å¥½åˆ—åº
            col_order = ['åŠ¨é”€SKUæ•°', 'è§’è‰²SKUæ±‡æ€»', 'ä»·æ ¼å¸¦SKUæ±‡æ€»', 'æ€»é”€å”®é¢(å»é‡å)', 'è§’è‰²é”€å”®é¢æ±‡æ€»', 'ä»·æ ¼å¸¦é”€å”®é¢æ±‡æ€»', 'æ ¡éªŒ_è§’è‰²SKUä¸€è‡´', 'æ ¡éªŒ_ä»·æ ¼å¸¦SKUä¸€è‡´', 'æ ¡éªŒ_è§’è‰²é”€å”®é¢ä¸€è‡´', 'æ ¡éªŒ_ä»·æ ¼å¸¦é”€å”®é¢ä¸€è‡´']
            exist_cols = [c for c in col_order if c in chk.columns]
            chk = chk[exist_cols]
            chk = apply_cn_columns(chk)
            # chk.to_excel(writer, sheet_name='æ ¡éªŒ-è§’è‰²ä¸ä»·æ ¼å¸¦ä¸€è‡´æ€§')  # ã€å·²ç¦ç”¨ã€‘ç”¨æˆ·è¦æ±‚åˆ é™¤æ­¤Sheet
        except Exception as ce:
            print(f"âš ï¸ ç”Ÿæˆâ€˜æ ¡éªŒ-è§’è‰²ä¸ä»·æ ¼å¸¦ä¸€è‡´æ€§â€™å¤±è´¥ï¼š{ce}")
        all_l1_analysis = pd.concat([
            res['ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡'].assign(é—¨åº—=store)
            for store, res in all_results.items() if 'ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡' in res
        ])
        all_l1_analysis = apply_cn_columns(all_l1_analysis)
        all_l1_analysis.to_excel(writer, sheet_name='ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡', index=False)
        
        # ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡
        l3_results = [
            res['ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡'].assign(é—¨åº—=store)
            for store, res in all_results.items() if 'ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡' in res
        ]
        if l3_results:
            all_l3_analysis = pd.concat(l3_results)
            all_l3_analysis = apply_cn_columns(all_l3_analysis)
            all_l3_analysis.to_excel(writer, sheet_name='ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡', index=False)
            print(f"â„¹ï¸ ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡Sheetå·²ç”Ÿæˆï¼ŒåŒ…å« {len(all_l3_analysis)} æ¡è®°å½•ã€‚")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°ä¸‰çº§åˆ†ç±»æ•°æ®ï¼Œè·³è¿‡ä¸‰çº§åˆ†ç±»Sheetç”Ÿæˆã€‚")
        
        all_deduplicated_dfs = pd.concat([data['deduplicated'] for data in all_store_data.values()], ignore_index=True)
        all_deduplicated_dfs = apply_cn_columns(all_deduplicated_dfs)
        all_deduplicated_dfs.to_excel(writer, sheet_name='è¯¦ç»†SKUæŠ¥å‘Š(å»é‡å)', index=False)
        
        all_skus_combined = pd.concat([data['all_skus'] for data in all_store_data.values()], ignore_index=True)
        multi_spec_report = identify_multi_spec_products(all_skus_combined)
        # å…ˆå®Œæˆæ‰€æœ‰ä½¿ç”¨è‹±æ–‡åˆ—åçš„æ“ä½œï¼Œå†åº”ç”¨ä¸­æ–‡åŒ–
        multi_spec_report_cn = apply_cn_columns(multi_spec_report)
        multi_spec_report_cn.to_excel(writer, sheet_name='å¤šè§„æ ¼å•†å“æŠ¥å‘Š(å…¨)', index=False)
        # SKUç»“æ„æ¦‚è§ˆï¼šæŒ‰é—¨åº—+base_name çš„å˜ä½“ç»“æ„
        try:
            sku_structure_rows = []
            for store, data in all_store_data.items():
                dfw = data['all_skus'].copy()
                dfw['base_name'] = dfw['product_name'].apply(_normalize_base_name)
                def _vk(row):
                    v = row.get('è§„æ ¼åç§°', None)
                    v = v if isinstance(v, str) and v.strip() != '' else None
                    if not v:
                        v = _extract_inferred_spec(row.get('product_name', ''))
                    if not v:
                        bc = row.get('barcode', None)
                        bc = str(bc).strip() if isinstance(bc, (int, float, str)) else None
                        if bc and bc.lower() not in ('nan', 'none'):
                            v = bc
                    return v
                dfw['variant_key'] = dfw.apply(_vk, axis=1)
                # å˜ä½“è®¡æ•°ä¸ç¤ºä¾‹
                g = dfw.groupby('base_name')['variant_key'].agg(['nunique', lambda x: ', '.join(pd.Series(x).dropna().astype(str).unique()[:5])]).reset_index()
                g.columns = ['base_name', 'å˜ä½“æ•°', 'ç¤ºä¾‹å˜ä½“(â‰¤5)']
                g['ç»“æ„ç±»å‹'] = np.where(g['å˜ä½“æ•°'] > 1, 'å¤šè§„æ ¼', 'å•è§„æ ¼')
                g['é—¨åº—'] = store
                sku_structure_rows.append(g)
            if sku_structure_rows:
                sku_structure_df = pd.concat(sku_structure_rows, ignore_index=True)
                sku_structure_df = sku_structure_df[['é—¨åº—', 'base_name', 'ç»“æ„ç±»å‹', 'å˜ä½“æ•°', 'ç¤ºä¾‹å˜ä½“(â‰¤5)']]
                # åº”ç”¨ä¸­æ–‡åŒ–åˆ—åæ˜ å°„
                sku_structure_df = sku_structure_df.rename(columns={'base_name': 'åŸºç¡€åç§°'})
                sku_structure_df.to_excel(writer, sheet_name='SKUç»“æ„æ¦‚è§ˆ', index=False)
        except Exception as se:
            print(f"âš ï¸ ç”ŸæˆSKUç»“æ„æ¦‚è§ˆå¤±è´¥ï¼š{se}")
        # ä¾›ä¸€è‡´æ€§æ ¡éªŒç”¨çš„â€œå»é‡åå¤šè§„æ ¼å˜ä½“è®¡æ•°â€ï¼šæ¯ä¸ª (Store, base_name) çš„ variant_key æ•°
        if not multi_spec_report.empty and all(col in multi_spec_report.columns for col in ['base_name', 'variant_key']):
            m_count_df = multi_spec_report.dropna(subset=['variant_key']).copy()
            g_keys = ['Store', 'base_name'] if 'Store' in m_count_df.columns else ['base_name']
            var_cnt = m_count_df.groupby(g_keys)['variant_key'].nunique().rename('è§„æ ¼ç§ç±»æ•°_æŒ‰å˜ä½“é”®').reset_index()
        else:
            var_cnt = pd.DataFrame()
        if not multi_spec_report.empty:
            # åŠ¨æ€é€‰æ‹©å¯ç”¨çš„å”¯ä¸€é”®ï¼šä¼˜å…ˆ product_nameï¼Œä¸å­˜åœ¨åˆ™å›é€€ base_name
            has_store = 'Store' in multi_spec_report.columns
            keys_candidates = []
            if has_store and 'product_name' in multi_spec_report.columns:
                keys_candidates = ['Store', 'product_name']
            elif has_store and 'base_name' in multi_spec_report.columns:
                keys_candidates = ['Store', 'base_name']
            elif 'product_name' in multi_spec_report.columns:
                keys_candidates = ['product_name']
            elif 'base_name' in multi_spec_report.columns:
                keys_candidates = ['base_name']
            else:
                # å…œåº•ï¼šä½¿ç”¨é¦–åˆ—
                keys_candidates = [multi_spec_report.columns[0]]

            # å…ˆå‡†å¤‡åˆ†ç±»å’Œæœˆå”®ä¿¡æ¯ï¼ˆä½¿ç”¨è‹±æ–‡åˆ—åï¼‰
            category_sales_info = None
            if not multi_spec_report.empty:
                # æ„å»ºèšåˆå­—å…¸ï¼ŒåªåŒ…å«å­˜åœ¨çš„åˆ—
                agg_dict = {}
                if 'l1_category' in multi_spec_report.columns:
                    agg_dict['l1_category'] = 'first'
                if 'l3_category' in multi_spec_report.columns:
                    agg_dict['l3_category'] = 'first'
                if 'sales_qty' in multi_spec_report.columns:
                    agg_dict['sales_qty'] = 'first'  # å ä½ï¼Œå®é™…ä¼šç‰¹æ®Šå¤„ç†
                # æ·»åŠ ä»·æ ¼å’Œåº“å­˜ç›¸å…³å­—æ®µ - ä¿®æ­£ï¼šæŒ‰æœ€ä½³ä»£è¡¨è§„æ ¼å–å€¼ï¼ˆå¤šçº§æ’åºï¼‰
                # å¯¹äºä»·æ ¼å’Œé”€å”®é¢ï¼Œä¸èƒ½ç”¨å¹³å‡å€¼ï¼Œåº”è¯¥ä»¥é”€é‡æœ€é«˜çš„è§„æ ¼ä¸ºå‡†
                # è¿™é‡Œå…ˆç”¨ 'first' ä½œä¸ºå ä½ï¼Œåé¢ä¼šç‰¹æ®Šå¤„ç†
                if 'price' in multi_spec_report.columns:
                    agg_dict['price'] = 'first'  # å ä½ï¼Œå®é™…ä¼šç‰¹æ®Šå¤„ç†
                if 'original_price' in multi_spec_report.columns:
                    agg_dict['original_price'] = 'first'  # å ä½ï¼Œå®é™…ä¼šç‰¹æ®Šå¤„ç†
                if 'åº“å­˜' in multi_spec_report.columns:
                    agg_dict['åº“å­˜'] = 'sum'  # åº“å­˜ä»ç„¶å–æ€»å’Œ
                # é”€å”®é¢å­—æ®µä¹Ÿéœ€è¦ç‰¹æ®Šå¤„ç†
                if 'revenue' in multi_spec_report.columns:
                    agg_dict['revenue'] = 'first'  # å ä½ï¼Œå®é™…ä¼šç‰¹æ®Šå¤„ç†
                if 'original_price_revenue' in multi_spec_report.columns:
                    agg_dict['original_price_revenue'] = 'first'  # å ä½ï¼Œå®é™…ä¼šç‰¹æ®Šå¤„ç†
                
                if agg_dict:  # åªåœ¨æœ‰å¯ç”¨åˆ—æ—¶è¿›è¡Œèšåˆ
                    category_sales_info = multi_spec_report.groupby(keys_candidates).agg(agg_dict).reset_index()
                    
                    # ç‰¹æ®Šå¤„ç†ï¼šå¯¹äºä»·æ ¼å’Œé”€å”®é¢å­—æ®µï¼Œå–æœ€ä½³ä»£è¡¨è§„æ ¼çš„æ•°æ®ï¼ˆå¤šçº§æ’åºï¼‰
                    price_revenue_fields = ['price', 'original_price', 'revenue', 'original_price_revenue']
                    existing_fields = [f for f in price_revenue_fields if f in multi_spec_report.columns]
                    
                    if existing_fields:
                        print(f"â„¹ï¸ æ­£åœ¨ä¸ºå¤šè§„æ ¼å•†å“é‡æ–°è®¡ç®—ä»·æ ¼å’Œé”€å”®é¢ï¼ˆå¤šçº§æ’åºé€‰æ‹©æœ€ä½³ä»£è¡¨è§„æ ¼ï¼‰...")
                        
                        # ä¸ºæ¯ä¸ªå•†å“ç»„æ‰¾åˆ°æœ€ä½³ä»£è¡¨è§„æ ¼
                        max_sales_data = []
                        for group_key in category_sales_info[keys_candidates].to_dict('records'):
                            # ä½¿ç”¨ç›´æ¥ç­›é€‰æ–¹æ³•ï¼Œé¿å… query å­—ç¬¦ä¸²æ„é€ é—®é¢˜
                            mask = pd.Series([True] * len(multi_spec_report))
                            for key, value in group_key.items():
                                mask = mask & (multi_spec_report[key] == value)
                            
                            product_variants = multi_spec_report[mask]
                            
                            if not product_variants.empty:
                                # å¤šçº§æ’åºé€‰æ‹©æœ€ä½³ä»£è¡¨è§„æ ¼ï¼šé”€é‡é™åºã€ä»·æ ¼å‡åºã€åº“å­˜é™åºã€è§„æ ¼åç§°å‡åº
                                sorted_variants = product_variants.sort_values(
                                    by=['sales_qty', 'price', 'åº“å­˜', 'è§„æ ¼åç§°'], 
                                    ascending=[False, True, False, True],
                                    na_position='last'
                                )
                                max_sales_row = sorted_variants.iloc[0]
                                
                                # ä¿å­˜è¯¥è§„æ ¼çš„ä»·æ ¼å’Œé”€å”®é¢æ•°æ®
                                max_sales_record = {**group_key}
                                for field in existing_fields:
                                    max_sales_record[field] = max_sales_row[field]
                                
                                max_sales_data.append(max_sales_record)
                        
                        # å°†ç‰¹æ®Šå¤„ç†çš„æ•°æ®åˆå¹¶å› category_sales_info
                        if max_sales_data:
                            max_sales_df = pd.DataFrame(max_sales_data)
                            
                            # ç”¨æ–°æ•°æ®æ›´æ–° category_sales_info ä¸­çš„å¯¹åº”å­—æ®µ
                            for field in existing_fields:
                                if field in max_sales_df.columns:
                                    # åˆ é™¤åŸæœ‰åˆ—ï¼Œç”¨æ–°æ•°æ®æ›¿æ¢
                                    category_sales_info = category_sales_info.drop(columns=[field], errors='ignore')
                                    category_sales_info = category_sales_info.merge(
                                        max_sales_df[keys_candidates + [field]], 
                                        on=keys_candidates, 
                                        how='left'
                                    )

            # ä½¿ç”¨è‹±æ–‡åˆ—åç¡®å®šè¦ä¿ç•™çš„åˆ—ï¼Œæ·»åŠ ä»·æ ¼ã€åº“å­˜å’Œé”€å”®é¢ç›¸å…³å­—æ®µ
            keep_cols_en = [c for c in ['Store', 'product_name', 'è§„æ ¼ç§ç±»æ•°', 'å¤šè§„æ ¼ä¾æ®', 'l1_category', 'l3_category', 'sales_qty', 'price', 'original_price', 'åº“å­˜', 'revenue', 'original_price_revenue'] if c in multi_spec_report.columns]
            
            # ä½¿ç”¨å¤šçº§æ’åºè¿›è¡Œå»é‡ï¼Œç¡®ä¿é€‰æ‹©æœ€ä½³ä»£è¡¨è§„æ ¼ï¼šé”€é‡é™åºã€ä»·æ ¼å‡åºã€åº“å­˜é™åºã€è§„æ ¼åç§°å‡åº
            unique_multi_spec_list = multi_spec_report.sort_values(
                by=['sales_qty', 'price', 'åº“å­˜', 'è§„æ ¼åç§°'], 
                ascending=[False, True, False, True],
                na_position='last'
            )
            unique_multi_spec_list = unique_multi_spec_list.drop_duplicates(subset=keys_candidates, keep='first')
            
            # åˆå¹¶åˆ†ç±»å’Œæœˆå”®ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if category_sales_info is not None and not category_sales_info.empty:
                unique_multi_spec_list = unique_multi_spec_list.merge(
                    category_sales_info, 
                    on=keys_candidates, 
                    how='left',
                    suffixes=('', '_agg')
                )
                # åˆ é™¤é‡å¤çš„èšåˆåˆ—ï¼ˆä¿ç•™åŸå§‹åˆ—ï¼‰
                for col in ['l1_category_agg', 'l3_category_agg', 'sales_qty_agg', 'price_agg', 'original_price_agg', 'åº“å­˜_agg', 'revenue_agg', 'original_price_revenue_agg']:
                    if col in unique_multi_spec_list.columns:
                        base_col = col.replace('_agg', '')
                        if base_col in unique_multi_spec_list.columns:
                            unique_multi_spec_list = unique_multi_spec_list.drop(columns=[col])
            
            # åªå¯¼å‡ºå…³é”®åˆ—ï¼Œè‹¥ç¼ºå¤±åˆ™å¯¼å‡ºå…¨é‡
            if keep_cols_en:
                available_cols = [c for c in keep_cols_en if c in unique_multi_spec_list.columns]
                if available_cols:
                    unique_multi_spec_list = unique_multi_spec_list[available_cols]
            
            # è¡¥å……ç¼ºå¤±çš„ä»·æ ¼ã€åº“å­˜å’Œé”€å”®é¢å­—æ®µï¼ˆå¦‚æœèšåˆä¸­æ²¡æœ‰è·å–åˆ°ï¼‰
            missing_fields = ['price', 'original_price', 'åº“å­˜', 'revenue', 'original_price_revenue']
            price_revenue_fields = ['price', 'original_price', 'revenue', 'original_price_revenue']
            
            for field in missing_fields:
                if field not in unique_multi_spec_list.columns and field in multi_spec_report.columns:
                    # å¯¹äºä»·æ ¼å’Œé”€å”®é¢å­—æ®µï¼Œå–æœ€ä½³ä»£è¡¨è§„æ ¼çš„æ•°æ®ï¼ˆå¤šçº§æ’åºï¼‰
                    if field in price_revenue_fields:
                        # æ‰¾åˆ°æ¯ä¸ªå•†å“ç»„ä¸­é”€é‡æœ€é«˜è§„æ ¼çš„æ•°æ®
                        field_values_list = []
                        for _, group_row in unique_multi_spec_list.iterrows():
                            group_key = {k: group_row[k] for k in keys_candidates if k in group_row.index}
                            
                            # ä½¿ç”¨ç›´æ¥ç­›é€‰æ–¹æ³•ï¼Œé¿å… query å­—ç¬¦ä¸²æ„é€ é—®é¢˜
                            mask = pd.Series([True] * len(multi_spec_report))
                            for key, value in group_key.items():
                                mask = mask & (multi_spec_report[key] == value)
                            
                            product_variants = multi_spec_report[mask]
                            
                            if not product_variants.empty:
                                # å¤šçº§æ’åºé€‰æ‹©æœ€ä½³ä»£è¡¨è§„æ ¼ï¼šé”€é‡é™åºã€ä»·æ ¼å‡åºã€åº“å­˜é™åºã€è§„æ ¼åç§°å‡åº
                                sorted_variants = product_variants.sort_values(
                                    by=['sales_qty', 'price', 'åº“å­˜', 'è§„æ ¼åç§°'], 
                                    ascending=[False, True, False, True],
                                    na_position='last'
                                )
                                max_sales_row = sorted_variants.iloc[0]
                                field_value = max_sales_row[field]
                            else:
                                field_value = 0
                                
                            field_record = {**group_key, field: field_value}
                            field_values_list.append(field_record)
                        
                        if field_values_list:
                            field_values = pd.DataFrame(field_values_list)
                            unique_multi_spec_list = unique_multi_spec_list.merge(
                                field_values, 
                                on=keys_candidates, 
                                how='left'
                            )
                    elif field == 'åº“å­˜':
                        # åº“å­˜ä»ç„¶å–æ€»å’Œ
                        field_values = multi_spec_report.groupby(keys_candidates)[field].sum().reset_index()
                        unique_multi_spec_list = unique_multi_spec_list.merge(
                            field_values, 
                            on=keys_candidates, 
                            how='left'
                        )
                    else:
                        # å…¶ä»–å­—æ®µå–é¦–ä¸ªå€¼
                        field_values = multi_spec_report.groupby(keys_candidates)[field].first().reset_index()
                        unique_multi_spec_list = unique_multi_spec_list.merge(
                            field_values, 
                            on=keys_candidates, 
                            how='left'
                        )
            
            # ç°åœ¨åº”ç”¨ä¸­æ–‡åŒ–åˆ—å
            unique_multi_spec_list = apply_cn_columns(unique_multi_spec_list)
            
            # è°ƒæ•´åˆ—é¡ºåºï¼Œå°†å…³é”®ä¿¡æ¯æ”¾åœ¨å‰é¢
            preferred_cols_order = ['é—¨åº—', 'å•†å“åç§°', 'ä¸€çº§åˆ†ç±»', 'ä¸‰çº§åˆ†ç±»', 'æœˆå”®', 'å”®ä»·', 'åŸä»·', 'å”®ä»·é”€å”®é¢', 'åŸä»·é”€å”®é¢', 'åº“å­˜', 'è§„æ ¼ç§ç±»æ•°', 'å¤šè§„æ ¼ä¾æ®']
            available_preferred_cols = [c for c in preferred_cols_order if c in unique_multi_spec_list.columns]
            remaining_cols = [c for c in unique_multi_spec_list.columns if c not in available_preferred_cols]
            if available_preferred_cols:
                unique_multi_spec_list = unique_multi_spec_list[available_preferred_cols + remaining_cols]
            
            # é—®é¢˜2ï¼šåˆ é™¤é‡å¤åˆ—ï¼ˆCã€Hã€Iåˆ—å¯¹åº”çš„å¯èƒ½æ˜¯åŸºç¡€åç§°ç­‰é‡å¤ä¿¡æ¯ï¼‰
            # åˆ é™¤å¯èƒ½é‡å¤çš„åˆ—
            duplicate_cols_to_remove = ['åŸºç¡€åç§°']  # åŸºäºå®é™…æƒ…å†µè°ƒæ•´
            for col in duplicate_cols_to_remove:
                if col in unique_multi_spec_list.columns:
                    unique_multi_spec_list = unique_multi_spec_list.drop(columns=[col])
            
            unique_multi_spec_list.to_excel(writer, sheet_name='å”¯ä¸€å¤šè§„æ ¼å•†å“åˆ—è¡¨', index=False)

            # === æ ¡éªŒï¼šKPI vs å”¯ä¸€å¤šè§„æ ¼å•†å“åˆ—è¡¨ ===
            try:
                # æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”ä¸­çš„å¤šè§„æ ¼æ•°
                kpi_multi = core_kpi_df[['å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°']].copy()
                kpi_multi = kpi_multi.reset_index().rename(columns={'index': 'é—¨åº—'})
                if 'é—¨åº—' not in kpi_multi.columns:
                    # è‹¥ç´¢å¼•åéâ€œé—¨åº—â€ï¼Œå°†ç¬¬ä¸€åˆ—è§†ä¸ºé—¨åº—
                    kpi_multi.columns = ['é—¨åº—'] + list(kpi_multi.columns[1:])

                # å”¯ä¸€å¤šè§„æ ¼å•†å“åˆ—è¡¨ä¸­çš„è®¡æ•° (å¥å£®ç‰ˆ) - ä½¿ç”¨ä¸­æ–‡åˆ—å
                list_multi_col = None
                if 'å•†å“åç§°' in unique_multi_spec_list.columns:
                    list_multi_col = 'å•†å“åç§°'
                elif 'åŸºç¡€åç§°' in unique_multi_spec_list.columns:
                    list_multi_col = 'åŸºç¡€åç§°'

                if 'é—¨åº—' in unique_multi_spec_list.columns and list_multi_col:
                    list_multi = unique_multi_spec_list.groupby('é—¨åº—')[list_multi_col].nunique().reset_index()
                    list_multi = list_multi.rename(columns={list_multi_col: 'å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°(åˆ—è¡¨)'})
                elif list_multi_col:
                    count = unique_multi_spec_list[list_multi_col].nunique()
                    store_name = kpi_multi['é—¨åº—'].iloc[0] if len(kpi_multi) > 0 else 'é—¨åº—A'
                    list_multi = pd.DataFrame({'é—¨åº—': [store_name], 'å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°(åˆ—è¡¨)': [count]})
                else:
                    # å¦‚æœä¸¤ä¸ªå…³é”®åˆ—éƒ½ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªç©ºçš„DataFrameä»¥é¿å…é”™è¯¯
                    list_multi = pd.DataFrame(columns=['é—¨åº—', 'å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°(åˆ—è¡¨)'])

                check_df = kpi_multi.merge(list_multi, on='é—¨åº—', how='outer')
                check_df['å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°'] = pd.to_numeric(check_df['å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°'], errors='coerce').fillna(0).astype(int)
                check_df['å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°(åˆ—è¡¨)'] = pd.to_numeric(check_df['å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°(åˆ—è¡¨)'], errors='coerce').fillna(0).astype(int)
                check_df['å·®å¼‚(åˆ—è¡¨-æŒ‡æ ‡)'] = check_df['å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°(åˆ—è¡¨)'] - check_df['å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°']

                # è¿›ä¸€æ­¥æ ¡éªŒï¼šè§„æ ¼ç§ç±»æ•°(æŒ‰å˜ä½“é”®)ä¹‹å’Œ vs å”¯ä¸€åˆ—è¡¨æ€»å’Œ
                if not var_cnt.empty:
                    if 'Store' in var_cnt.columns:
                        var_sum = var_cnt.groupby('Store')['è§„æ ¼ç§ç±»æ•°_æŒ‰å˜ä½“é”®'].sum().reset_index().rename(columns={'Store':'é—¨åº—','è§„æ ¼ç§ç±»æ•°_æŒ‰å˜ä½“é”®':'è§„æ ¼ç§ç±»æ•°åˆè®¡(å˜ä½“)'} )
                    else:
                        var_sum = pd.DataFrame({'é—¨åº—': [check_df['é—¨åº—'].iloc[0] if len(check_df)>0 else 'é—¨åº—A'], 'è§„æ ¼ç§ç±»æ•°åˆè®¡(å˜ä½“)': [int(var_cnt['è§„æ ¼ç§ç±»æ•°_æŒ‰å˜ä½“é”®'].sum())]})
                    check_df = check_df.merge(var_sum, on='é—¨åº—', how='left')

                # æ·»åŠ æ ·ä¾‹ï¼šä»…åœ¨KPI/ä»…åœ¨åˆ—è¡¨ (å¥å£®ç‰ˆ)
                samples_only_kpi = []
                samples_only_list = []

                # è¾…åŠ©å‡½æ•°ï¼Œå®‰å…¨åœ°è·å–ç”¨äºæ¯”è¾ƒçš„åç§°é›†åˆ
                def get_name_set(df, store_filter=None, use_chinese_cols=False):
                    if df is None or df.empty:
                        return set()
                    
                    # æ ¹æ®åˆ—åç±»å‹é€‰æ‹©æ­£ç¡®çš„åˆ—å
                    if use_chinese_cols:
                        store_col = 'é—¨åº—'
                        product_col = 'å•†å“åç§°'
                        base_col = 'åŸºç¡€åç§°'
                    else:
                        store_col = 'Store'
                        product_col = 'product_name'
                        base_col = 'base_name'
                    
                    # å¦‚æœæœ‰é—¨åº—ç­›é€‰ï¼Œå…ˆåº”ç”¨
                    if store_filter and store_col in df.columns:
                        df = df[df[store_col] == store_filter]

                    if product_col in df.columns:
                        return set(df[product_col].unique())
                    elif base_col in df.columns:
                        return set(df[base_col].unique())
                    return set()

                for _, row in check_df.iterrows():
                    store = row['é—¨åº—']
                    
                    # KPI é›†åˆ (ä½¿ç”¨è‹±æ–‡åˆ—å)
                    ms_kpi_df = pd.DataFrame()
                    if store in all_store_data:
                        ms_kpi_df = identify_multi_spec_products(all_store_data[store]['all_skus'])
                    set_kpi = get_name_set(ms_kpi_df, use_chinese_cols=False)

                    # åˆ—è¡¨é›†åˆ (ä½¿ç”¨ä¸­æ–‡åˆ—å)
                    set_list = get_name_set(unique_multi_spec_list, store_filter=store, use_chinese_cols=True)

                    only_kpi = list(set_kpi - set_list)[:5]
                    only_list = list(set_list - set_kpi)[:5]
                    samples_only_kpi.append(', '.join(map(str, only_kpi)))
                    samples_only_list.append(', '.join(map(str, only_list)))

                check_df['ç¤ºä¾‹ä»…åœ¨KPIä¸­(â‰¤5)'] = samples_only_kpi
                check_df['ç¤ºä¾‹ä»…åœ¨åˆ—è¡¨ä¸­(â‰¤5)'] = samples_only_list
                check_df = apply_cn_columns(check_df)
                # check_df.to_excel(writer, sheet_name='æ ¡éªŒ-å¤šè§„æ ¼ä¸€è‡´æ€§', index=False)  # ã€å·²ç¦ç”¨ã€‘ç”¨æˆ·è¦æ±‚åˆ é™¤æ­¤Sheet
            except Exception as ve:
                print(f"âš ï¸ ç”Ÿæˆâ€˜æ ¡éªŒ-å¤šè§„æ ¼ä¸€è‡´æ€§â€™å¤±è´¥ï¼š{ve}")

        # åº”ç”¨åˆ—æ ¼å¼ï¼šæ ¸å¿ƒæŒ‡æ ‡ï¼ˆæ•´æ•°/é‡‘é¢/ç™¾åˆ†æ¯”ï¼‰+ å…¶å®ƒ Sheet çš„ç™¾åˆ†æ¯”ç»Ÿä¸€ä¸º 0.00%
        try:
            ws = writer.sheets.get('æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”')
            if ws is not None:
                int_cols = [
                    'æ€»SKUæ•°(å«è§„æ ¼)', 'æ€»SKUæ•°(å»é‡å)', 'åŠ¨é”€SKUæ•°', 'æ»é”€SKUæ•°', 'å”¯ä¸€å¤šè§„æ ¼å•†å“æ•°'
                ]
                money_cols = ['æ€»é”€å”®é¢(å»é‡å)']
                pct_cols = ['åŠ¨é”€ç‡']

                if engine_name == 'xlsxwriter':
                    wb = writer.book
                    fmt_int = wb.add_format({'num_format': '0'})
                    fmt_money = wb.add_format({'num_format': '#,##0.00'})
                    fmt_pct = wb.add_format({'num_format': '0.00%'})
                    # åç§» = ç´¢å¼•å±‚çº§æ•°
                    offset = core_kpi_df.index.nlevels
                    def idx_of(col_name):
                        try:
                            return int(core_kpi_df.columns.get_loc(col_name)) + offset
                        except Exception:
                            return None
                    for name in int_cols:
                        ci = idx_of(name)
                        if ci is not None:
                            ws.set_column(ci, ci, None, fmt_int)
                    for name in money_cols:
                        ci = idx_of(name)
                        if ci is not None:
                            ws.set_column(ci, ci, None, fmt_money)
                    for name in pct_cols:
                        ci = idx_of(name)
                        if ci is not None:
                            ws.set_column(ci, ci, None, fmt_pct)
                else:  # openpyxl
                    # è¯»å–è¡¨å¤´æ˜ å°„ï¼švalue->column index
                    header_map = {}
                    for cell in ws[1]:
                        header_map[str(cell.value)] = cell.column
                    max_row = ws.max_row
                    def apply_number_format(col_names, fmt):
                        for n in col_names:
                            c = header_map.get(n)
                            if c is None:
                                continue
                            for r in range(2, max_row + 1):
                                cell = ws.cell(row=r, column=c)
                                cell.number_format = fmt
                    apply_number_format(int_cols, '0')
                    apply_number_format(money_cols, '#,##0.00')
                    apply_number_format(pct_cols, '0.00%')

            # å…¶å®ƒ Sheetï¼šç»Ÿä¸€ç™¾åˆ†æ¯”æ ¼å¼ï¼ˆæŒ‰ç™½åå•/æ•°å€¼åŸŸåˆ¤å®šï¼‰
            # å•†å“è§’è‰²åˆ†æ
            ws_role = writer.sheets.get('å•†å“è§’è‰²åˆ†æ')
            # ä»·æ ¼å¸¦åˆ†æ
            ws_price = writer.sheets.get('ä»·æ ¼å¸¦åˆ†æ')
            # ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡
            ws_l1 = writer.sheets.get('ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡')
            # ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡
            ws_l3 = writer.sheets.get('ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡')

            if engine_name == 'xlsxwriter':
                apply_pct_xlsxwriter(ws_role, role_df, get_sheet_pct_cols('å•†å“è§’è‰²åˆ†æ', role_df), index_written=True)
                apply_pct_xlsxwriter(ws_price, price_df, get_sheet_pct_cols('ä»·æ ¼å¸¦åˆ†æ', price_df), index_written=True)
                apply_pct_xlsxwriter(ws_l1, all_l1_analysis, get_sheet_pct_cols('ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡', all_l1_analysis), index_written=False)
                if 'all_l3_analysis' in locals() and not all_l3_analysis.empty:
                    apply_pct_xlsxwriter(ws_l3, all_l3_analysis, get_sheet_pct_cols('ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡', all_l3_analysis), index_written=False)

                # å¼ºåˆ¶æ•´æ•°æ ¼å¼ï¼šL1æ˜ç»†ä¸­çš„è®¡æ•°å­—æ®µ
                if ws_l1 is not None:
                    wb = writer.book
                    fmt_int2 = wb.add_format({'num_format': '#,##0'})
                    fmt_discount_zhe = wb.add_format({'num_format': '0.0"æŠ˜"'})
                    int_cols_l1 = [
                        'ç¾å›¢ä¸€çº§åˆ†ç±»skuæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»å¤šè§„æ ¼SKUæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»å¤šè§„æ ¼SPUæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»0åº“å­˜æ•°',
                        'æœˆå”®',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»åŠ¨é”€skuæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨skuæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»çˆ†å“skuæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»æŠ˜æ‰£skuæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»å»é‡SKUæ•°(å£å¾„åŒåŠ¨é”€ç‡)',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨å»é‡SKUæ•°(å£å¾„åŒå æ¯”)'
                    ]
                    # æ­¤ Sheet å†™å…¥æ—¶æ²¡æœ‰ç´¢å¼•åˆ—ï¼Œåç§»=0
                    for name in int_cols_l1:
                        if name in all_l1_analysis.columns:
                            ci = int(all_l1_analysis.columns.get_loc(name))
                            ws_l1.set_column(ci, ci, None, fmt_int2)
                    # æŠ˜æ‰£åˆ—æ ¼å¼åŒ–ä¸º x.x æŠ˜
                    if 'ç¾å›¢ä¸€çº§åˆ†ç±»æŠ˜æ‰£' in all_l1_analysis.columns:
                        ci = int(all_l1_analysis.columns.get_loc('ç¾å›¢ä¸€çº§åˆ†ç±»æŠ˜æ‰£'))
                        ws_l1.set_column(ci, ci, None, fmt_discount_zhe)
                
                # å¼ºåˆ¶æ•´æ•°æ ¼å¼ï¼šL3æ˜ç»†ä¸­çš„è®¡æ•°å­—æ®µ
                if ws_l3 is not None and 'all_l3_analysis' in locals() and not all_l3_analysis.empty:
                    wb = writer.book
                    fmt_int2 = wb.add_format({'num_format': '#,##0'})
                    fmt_discount_zhe = wb.add_format({'num_format': '0.0"æŠ˜"'})
                    int_cols_l3 = [
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»skuæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»å¤šè§„æ ¼SKUæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»å¤šè§„æ ¼SPUæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»0åº“å­˜æ•°',
                        'æœˆå”®',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»åŠ¨é”€skuæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨skuæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»çˆ†å“skuæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»æŠ˜æ‰£skuæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»å»é‡SKUæ•°(å£å¾„åŒåŠ¨é”€ç‡)',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨å»é‡SKUæ•°(å£å¾„åŒå æ¯”)'
                    ]
                    for name in int_cols_l3:
                        if name in all_l3_analysis.columns:
                            ci = int(all_l3_analysis.columns.get_loc(name))
                            ws_l3.set_column(ci, ci, None, fmt_int2)
                    # æŠ˜æ‰£åˆ—æ ¼å¼åŒ–ä¸º x.x æŠ˜
                    if 'ç¾å›¢ä¸‰çº§åˆ†ç±»æŠ˜æ‰£' in all_l3_analysis.columns:
                        ci = int(all_l3_analysis.columns.get_loc('ç¾å›¢ä¸‰çº§åˆ†ç±»æŠ˜æ‰£'))
                        ws_l3.set_column(ci, ci, None, fmt_discount_zhe)
            else:
                apply_pct_openpyxl(ws_role, get_sheet_pct_cols('å•†å“è§’è‰²åˆ†æ', role_df))
                apply_pct_openpyxl(ws_price, get_sheet_pct_cols('ä»·æ ¼å¸¦åˆ†æ', price_df))
                apply_pct_openpyxl(ws_l1, get_sheet_pct_cols('ç¾å›¢ä¸€çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡', all_l1_analysis))
                if 'all_l3_analysis' in locals() and not all_l3_analysis.empty:
                    apply_pct_openpyxl(ws_l3, get_sheet_pct_cols('ç¾å›¢ä¸‰çº§åˆ†ç±»è¯¦ç»†æŒ‡æ ‡', all_l3_analysis))

                # å¼ºåˆ¶æ•´æ•°æ ¼å¼ï¼šL1æ˜ç»†ä¸­çš„è®¡æ•°å­—æ®µï¼ˆopenpyxlï¼‰
                if ws_l1 is not None:
                    header_map = {str(cell.value): cell.column for cell in ws_l1[1]}
                    max_row = ws_l1.max_row
                    int_cols_l1 = [
                        'ç¾å›¢ä¸€çº§åˆ†ç±»skuæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»å¤šè§„æ ¼SKUæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»å¤šè§„æ ¼SPUæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»0åº“å­˜æ•°',
                        'æœˆå”®',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»åŠ¨é”€skuæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨skuæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»çˆ†å“skuæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»æŠ˜æ‰£skuæ•°',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»å»é‡SKUæ•°(å£å¾„åŒåŠ¨é”€ç‡)',
                        'ç¾å›¢ä¸€çº§åˆ†ç±»æ´»åŠ¨å»é‡SKUæ•°(å£å¾„åŒå æ¯”)'
                    ]
                    for name in int_cols_l1:
                        c = header_map.get(name)
                        if c is None:
                            continue
                        for r in range(2, max_row + 1):
                            ws_l1.cell(row=r, column=c).number_format = '#,##0'
                    # æŠ˜æ‰£åˆ—è®¾ç½®ä¸º 0.0"æŠ˜"
                    if 'ç¾å›¢ä¸€çº§åˆ†ç±»æŠ˜æ‰£' in header_map:
                        c = header_map['ç¾å›¢ä¸€çº§åˆ†ç±»æŠ˜æ‰£']
                        for r in range(2, max_row + 1):
                            ws_l1.cell(row=r, column=c).number_format = '0.0"æŠ˜"'
                
                # å¼ºåˆ¶æ•´æ•°æ ¼å¼ï¼šL3æ˜ç»†ä¸­çš„è®¡æ•°å­—æ®µï¼ˆopenpyxlï¼‰
                if ws_l3 is not None and 'all_l3_analysis' in locals() and not all_l3_analysis.empty:
                    header_map_l3 = {str(cell.value): cell.column for cell in ws_l3[1]}
                    max_row_l3 = ws_l3.max_row
                    int_cols_l3 = [
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»skuæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»å¤šè§„æ ¼SKUæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»å¤šè§„æ ¼SPUæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»0åº“å­˜æ•°',
                        'æœˆå”®',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»åŠ¨é”€skuæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨skuæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»çˆ†å“skuæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»æŠ˜æ‰£skuæ•°',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»å»é‡SKUæ•°(å£å¾„åŒåŠ¨é”€ç‡)',
                        'ç¾å›¢ä¸‰çº§åˆ†ç±»æ´»åŠ¨å»é‡SKUæ•°(å£å¾„åŒå æ¯”)'
                    ]
                    for name in int_cols_l3:
                        c = header_map_l3.get(name)
                        if c is None:
                            continue
                        for r in range(2, max_row_l3 + 1):
                            ws_l3.cell(row=r, column=c).number_format = '#,##0'
                    # æŠ˜æ‰£åˆ—è®¾ç½®ä¸º 0.0"æŠ˜"
                    if 'ç¾å›¢ä¸‰çº§åˆ†ç±»æŠ˜æ‰£' in header_map_l3:
                        c = header_map_l3['ç¾å›¢ä¸‰çº§åˆ†ç±»æŠ˜æ‰£']
                        for r in range(2, max_row_l3 + 1):
                            ws_l3.cell(row=r, column=c).number_format = '0.0"æŠ˜"'
            
            # ä¸ºå”¯ä¸€å¤šè§„æ ¼å•†å“åˆ—è¡¨è®¾ç½®æ•°å€¼æ ¼å¼
            ws_multi_unique = writer.sheets.get('å”¯ä¸€å¤šè§„æ ¼å•†å“åˆ—è¡¨')
            if ws_multi_unique is not None:
                if engine_name == 'xlsxwriter':
                    wb = writer.book
                    fmt_price = wb.add_format({'num_format': '0.00'})
                    fmt_int = wb.add_format({'num_format': '0'})
                    
                    # ä¸ºå”®ä»·ã€åŸä»·è®¾ç½®ä»·æ ¼æ ¼å¼ï¼Œä¸ºé”€å”®é¢è®¾ç½®è´§å¸æ ¼å¼ï¼Œä¸ºæœˆå”®ã€åº“å­˜è®¾ç½®æ•´æ•°æ ¼å¼
                    fmt_money = wb.add_format({'num_format': '#,##0.00'})
                    
                    price_cols = ['å”®ä»·', 'åŸä»·']
                    money_cols = ['å”®ä»·é”€å”®é¢', 'åŸä»·é”€å”®é¢']
                    int_cols = ['æœˆå”®', 'åº“å­˜', 'è§„æ ¼ç§ç±»æ•°']
                    
                    for col_name in price_cols:
                        if col_name in unique_multi_spec_list.columns:
                            try:
                                col_idx = int(unique_multi_spec_list.columns.get_loc(col_name))
                                ws_multi_unique.set_column(col_idx, col_idx, None, fmt_price)
                            except Exception:
                                pass
                    
                    for col_name in money_cols:
                        if col_name in unique_multi_spec_list.columns:
                            try:
                                col_idx = int(unique_multi_spec_list.columns.get_loc(col_name))
                                ws_multi_unique.set_column(col_idx, col_idx, None, fmt_money)
                            except Exception:
                                pass
                    
                    for col_name in int_cols:
                        if col_name in unique_multi_spec_list.columns:
                            try:
                                col_idx = int(unique_multi_spec_list.columns.get_loc(col_name))
                                ws_multi_unique.set_column(col_idx, col_idx, None, fmt_int)
                            except Exception:
                                pass
                else:  # openpyxl
                    header_map_unique = {str(cell.value): cell.column for cell in ws_multi_unique[1]}
                    max_row_unique = ws_multi_unique.max_row
                    
                    # ä»·æ ¼æ ¼å¼
                    price_cols = ['å”®ä»·', 'åŸä»·']
                    for col_name in price_cols:
                        c = header_map_unique.get(col_name)
                        if c is not None:
                            for r in range(2, max_row_unique + 1):
                                ws_multi_unique.cell(row=r, column=c).number_format = '0.00'
                    
                    # é”€å”®é¢æ ¼å¼ï¼ˆè´§å¸æ ¼å¼ï¼‰
                    money_cols = ['å”®ä»·é”€å”®é¢', 'åŸä»·é”€å”®é¢']
                    for col_name in money_cols:
                        c = header_map_unique.get(col_name)
                        if c is not None:
                            for r in range(2, max_row_unique + 1):
                                ws_multi_unique.cell(row=r, column=c).number_format = '#,##0.00'
                    
                    # æ•´æ•°æ ¼å¼
                    int_cols = ['æœˆå”®', 'åº“å­˜', 'è§„æ ¼ç§ç±»æ•°']
                    for col_name in int_cols:
                        c = header_map_unique.get(col_name)
                        if c is not None:
                            for r in range(2, max_row_unique + 1):
                                ws_multi_unique.cell(row=r, column=c).number_format = '0'
        except Exception as fe:
            print(f"âš ï¸ KPIåˆ—æ ¼å¼è®¾ç½®å¤±è´¥ï¼š{fe}")
        
        # ========== å¯¼å‡ºæˆæœ¬åˆ†æç›¸å…³Sheetï¼ˆæ–°å¢ï¼‰ ==========
        try:
            # åˆå¹¶æ‰€æœ‰é—¨åº—çš„æˆæœ¬åˆ†ææ•°æ®
            cost_summary_list = []
            high_margin_list = []
            low_margin_list = []
            
            for store, res in all_results.items():
                if 'æˆæœ¬åˆ†ææ±‡æ€»' in res:
                    df_cost = res['æˆæœ¬åˆ†ææ±‡æ€»'].copy()
                    df_cost.insert(0, 'é—¨åº—', store)
                    cost_summary_list.append(df_cost)
                
                if 'é«˜æ¯›åˆ©å•†å“TOP50' in res:
                    df_high = res['é«˜æ¯›åˆ©å•†å“TOP50'].copy()
                    df_high.insert(0, 'é—¨åº—', store)
                    high_margin_list.append(df_high)
                
                if 'ä½æ¯›åˆ©é¢„è­¦å•†å“' in res:
                    df_low = res['ä½æ¯›åˆ©é¢„è­¦å•†å“'].copy()
                    df_low.insert(0, 'é—¨åº—', store)
                    low_margin_list.append(df_low)
            
            # å¯¼å‡ºæˆæœ¬åˆ†ææ±‡æ€»
            if cost_summary_list:
                cost_summary_combined = pd.concat(cost_summary_list, ignore_index=True)
                cost_summary_combined = apply_cn_columns(cost_summary_combined)
                cost_summary_combined.to_excel(writer, sheet_name='æˆæœ¬åˆ†ææ±‡æ€»', index=False)
                
                # åº”ç”¨ç™¾åˆ†æ¯”æ ¼å¼
                ws_cost = writer.sheets.get('æˆæœ¬åˆ†ææ±‡æ€»')
                if ws_cost and engine_name == 'xlsxwriter':
                    apply_pct_xlsxwriter(ws_cost, cost_summary_combined, get_sheet_pct_cols('æˆæœ¬åˆ†ææ±‡æ€»', cost_summary_combined), index_written=False)
                elif ws_cost and engine_name == 'openpyxl':
                    apply_pct_openpyxl(ws_cost, get_sheet_pct_cols('æˆæœ¬åˆ†ææ±‡æ€»', cost_summary_combined))
                
                print(f"â„¹ï¸ æˆæœ¬åˆ†ææ±‡æ€»Sheetå·²ç”Ÿæˆ")
            
            # å¯¼å‡ºé«˜æ¯›åˆ©å•†å“TOP50
            if high_margin_list:
                high_margin_combined = pd.concat(high_margin_list, ignore_index=True)
                high_margin_combined = apply_cn_columns(high_margin_combined)
                high_margin_combined.to_excel(writer, sheet_name='é«˜æ¯›åˆ©å•†å“TOP50', index=False)
                
                # åº”ç”¨ç™¾åˆ†æ¯”æ ¼å¼
                ws_high = writer.sheets.get('é«˜æ¯›åˆ©å•†å“TOP50')
                if ws_high and engine_name == 'xlsxwriter':
                    apply_pct_xlsxwriter(ws_high, high_margin_combined, get_sheet_pct_cols('é«˜æ¯›åˆ©å•†å“TOP50', high_margin_combined), index_written=False)
                elif ws_high and engine_name == 'openpyxl':
                    apply_pct_openpyxl(ws_high, get_sheet_pct_cols('é«˜æ¯›åˆ©å•†å“TOP50', high_margin_combined))
                
                print(f"â„¹ï¸ é«˜æ¯›åˆ©å•†å“TOP50Sheetå·²ç”Ÿæˆ")
            
            # å¯¼å‡ºä½æ¯›åˆ©é¢„è­¦å•†å“
            if low_margin_list:
                low_margin_combined = pd.concat(low_margin_list, ignore_index=True)
                low_margin_combined = apply_cn_columns(low_margin_combined)
                low_margin_combined.to_excel(writer, sheet_name='ä½æ¯›åˆ©é¢„è­¦å•†å“', index=False)
                
                # åº”ç”¨ç™¾åˆ†æ¯”æ ¼å¼
                ws_low = writer.sheets.get('ä½æ¯›åˆ©é¢„è­¦å•†å“')
                if ws_low and engine_name == 'xlsxwriter':
                    apply_pct_xlsxwriter(ws_low, low_margin_combined, get_sheet_pct_cols('ä½æ¯›åˆ©é¢„è­¦å•†å“', low_margin_combined), index_written=False)
                elif ws_low and engine_name == 'openpyxl':
                    apply_pct_openpyxl(ws_low, get_sheet_pct_cols('ä½æ¯›åˆ©é¢„è­¦å•†å“', low_margin_combined))
                
                print(f"â„¹ï¸ ä½æ¯›åˆ©é¢„è­¦å•†å“Sheetå·²ç”Ÿæˆ")
        
        except Exception as ce:
            print(f"âš ï¸ å¯¼å‡ºæˆæœ¬åˆ†æSheetå¤±è´¥ï¼š{ce}")
            import traceback
            traceback.print_exc()
    
    print(f"âœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼å·²ä¿å­˜è‡³: '{output_path}'ã€‚")

# ----------------------------------------
# 4. ä¸»æ‰§è¡Œæµç¨‹ (v3.4 äº¤äº’å¼)
# ----------------------------------------
def parse_args():
    parser = argparse.ArgumentParser(description="é—¨åº—åŸºç¡€æ•°æ®åˆ†æï¼ˆæœ¬åœ°è¿è¡Œç‰ˆï¼Œä¿æŒåŸæœ‰é€»è¾‘ï¼‰")
    parser.add_argument("--inputs", nargs='*', help="æŒ‰ STORES_TO_ANALYZE é¡ºåºæä¾›æ¯ä¸ªé—¨åº—çš„æ–‡ä»¶è·¯å¾„ (.csv/.xlsx)")
    parser.add_argument("--output", help="è¾“å‡º Excel æ–‡ä»¶åæˆ–å®Œæ•´è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨è„šæœ¬å†…é…ç½®ï¼‰")
    parser.add_argument("--output-dir", help="è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼Œé»˜è®¤å†™å…¥è„šæœ¬åŒç›®å½•çš„ reports/ï¼‰")
    return parser.parse_args()


if __name__ == "__main__":
    # --- é…ç½®åŒº ---
    # æ£€æŸ¥æ˜¯å¦ä¸ºè‡ªåŠ¨æµ‹è¯•æ¨¡å¼
    AUTO_TEST = len(sys.argv) > 1 and sys.argv[1] == "--auto-test"
    
    if AUTO_TEST:
        # è‡ªåŠ¨æµ‹è¯•æ¨¡å¼é…ç½®
        STORES_TO_ANALYZE = ["æƒ å®œé€‰æµ‹è¯•åº—"]
        test_file = Path(__file__).parent.parent / "æƒ å®œé€‰.xlsx"
        if not test_file.exists():
            print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            sys.exit(1)
        print(f"ğŸ§ª è‡ªåŠ¨æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨æ–‡ä»¶: {test_file.name}")
    else:
        # æ­£å¸¸äº¤äº’æ¨¡å¼é…ç½®
        STORES_TO_ANALYZE = [
            "å¯ä»¥é€‰"
            # "æ¾é¼ ä¾¿åˆ©",
            # "é—¨åº—D",
        ]
    OUTPUT_FILENAME = "ç«å¯¹åˆ†ææŠ¥å‘Š_v3.4_FINAL.xlsx"
    CONSUMPTION_SCENARIOS = {
        "æ—©é¤å¿«æ‰‹": ["æ—©é¤", "ç‰›å¥¶", "é¢åŒ…", "éº¦ç‰‡", "é¸¡è›‹"],
        "åŠ ç­èƒ½é‡è¡¥ç»™": ["å’–å•¡", "èƒ½é‡é¥®æ–™", "å·§å…‹åŠ›", "é¥¼å¹²", "èƒ½é‡æ£’"],
        "å®¶åº­å›¤è´§": ["å¤§åŒ…è£…", "å®¶åº­è£…", "ç»„åˆè£…", "ç®±", "é‡è´©"],
        "èšä¼šé›¶é£Ÿ": ["è–¯ç‰‡", "è†¨åŒ–", "ç³–æœ", "åšæœ", "æ±½æ°´", "å•¤é…’"],
    }
    # --- é…ç½®ç»“æŸ ---

    args = parse_args()
    # è®¡ç®—è¾“å‡ºè·¯å¾„ï¼šé»˜è®¤å†™å…¥è„šæœ¬ç›®å½•ä¸‹çš„ reports/
    script_dir = Path(__file__).parent.resolve()
    default_out_dir = script_dir / "reports"
    out_dir = Path(args.output_dir).resolve() if getattr(args, 'output_dir', None) else default_out_dir

    # ç¡®å®šè¾“å‡ºæ–‡ä»¶åï¼š
    if args.output:
        user_out = Path(args.output)
        if user_out.is_absolute() or str(user_out.parent) not in (".", ""):
            final_output_path = user_out
        else:
            final_output_path = out_dir / user_out.name
    else:
        final_output_path = out_dir / OUTPUT_FILENAME

    print("ğŸš€ æ¬¢è¿ä½¿ç”¨å…¨ç»´åº¦ç«å¯¹åˆ†æå¼•æ“ v3.4 (æœ¬åœ°è¿è¡Œç‰ˆ)")

    all_store_results = {}
    all_processed_data = {}

    for idx, store_name in enumerate(STORES_TO_ANALYZE, start=1):
        print("-" * 50)
        print(f"æ­¥éª¤ {idx}/{len(STORES_TO_ANALYZE)}: ä¸ºã€{store_name}ã€‘æä¾›æ•°æ®æ–‡ä»¶ (æ”¯æŒ .csv æˆ– .xlsx)")
        try:
            if AUTO_TEST and store_name == "æƒ å®œé€‰æµ‹è¯•åº—":
                # è‡ªåŠ¨æµ‹è¯•æ¨¡å¼ä½¿ç”¨é¢„è®¾æ–‡ä»¶
                file_path = str(test_file)
            elif args.inputs and len(args.inputs) >= idx:
                file_path = args.inputs[idx - 1]
            else:
                print(f"\nğŸ’¡ æç¤º: ç›´æ¥æ‹–æ‹½Excelæ–‡ä»¶åˆ°ç»ˆç«¯,ç„¶åæŒ‰å›è½¦å³å¯")
                print(f"   (PowerShellç”¨æˆ·: æ‹–æ‹½åä¼šè‡ªåŠ¨æ·»åŠ  '& ' å‰ç¼€,æ— éœ€æ‰‹åŠ¨åˆ é™¤)")
                print(f"   æˆ–æ‰‹åŠ¨è¾“å…¥æ–‡ä»¶è·¯å¾„:")
                file_path = input(f"ã€{store_name}ã€‘æ–‡ä»¶è·¯å¾„: ").strip()
                
                # å¤„ç†Windowsè·¯å¾„ä¸­çš„å¼•å·ã€ç©ºæ ¼å’ŒPowerShellå‘½ä»¤ç¬¦å·
                file_path = file_path.strip()
                # ç§»é™¤PowerShellçš„å‘½ä»¤æ‰§è¡Œç¬¦å· & 
                if file_path.startswith('& '):
                    file_path = file_path[2:].strip()
                # ç§»é™¤å¤–å±‚å¼•å·
                file_path = file_path.strip('"').strip("'").strip()
            
            if not file_path:
                print(f"âš ï¸ æœªæä¾›æ–‡ä»¶è·¯å¾„ï¼Œè·³è¿‡åº—é“º: {store_name}")
                continue

            processed = load_and_clean_data(file_path, store_name, CONSUMPTION_SCENARIOS)
            if processed and not processed[1].empty:
                df_all, df_dedup, df_act = processed
                all_processed_data[store_name] = {
                    'all_skus': df_all,
                    'deduplicated': df_dedup,
                    'active': df_act
                }
                analysis_results = analyze_store_performance(df_all, df_dedup, df_act)
                if analysis_results:
                    all_store_results[store_name] = analysis_results
        except Exception as e:
            print(f"âŒ å¤„ç†åº—é“º {store_name} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            traceback.print_exc()

    if all_store_results:
        export_full_report_to_excel(all_store_results, all_processed_data, str(final_output_path))
        if len(all_store_results) > 1:
            print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...")
            # TODO: å¦‚éœ€ï¼Œå¯åœ¨æ­¤è¡¥å……å›¾è¡¨è¾“å‡ºé€»è¾‘
    else:
        print("\nâ¹ï¸ æ²¡æœ‰å¯ä¾›åˆ†æçš„æ•°æ®ï¼Œç¨‹åºç»“æŸã€‚")